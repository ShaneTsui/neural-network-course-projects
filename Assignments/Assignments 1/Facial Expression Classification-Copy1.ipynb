{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure code for dataset building  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_image_dict= {}\n",
    "    \n",
    "    def add(self, image, label):\n",
    "        self.label_image_dict[label] = image\n",
    "    \n",
    "    def get(self, label):\n",
    "        return self.label_image_dict[label]\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, data=[], labels=[]):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def to_numpy_array(self):\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "    def insert(self, datum, label):\n",
    "        self.data.append(datum)\n",
    "        self.labels.append(label)\n",
    "    \n",
    "    def extend(self, data, labels):\n",
    "        self.data.extend(data)\n",
    "        self.labels.extend(labels)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        idx = np.array(list(range(len(self.data))))\n",
    "        np.random.shuffle(idx)\n",
    "        self.data[:] = self.data[idx]\n",
    "        self.labels[:] = self.labels[idx]\n",
    "    \n",
    "class DataBuilder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.subjects = defaultdict(Subject)\n",
    "    \n",
    "    def get_subject_ids(self):\n",
    "        return list(self.subjects.keys())\n",
    "    \n",
    "    def load_data(self, data_dir=\"./CAFE/\"):\n",
    "        # Get the list of image file names\n",
    "        all_files = listdir(data_dir)\n",
    "        \n",
    "        # Store the images and labels in self.subjects dictionary\n",
    "        for file in all_files:\n",
    "            # Load in the files as PIL images and convert to NumPy arrays\n",
    "            subject, rest_string = file.split('_')\n",
    "            label = rest_string.split('.')[0][:-1]\n",
    "            \n",
    "            # Exclude neutral and happy faces \n",
    "            if label != 'n' and label != 'h':\n",
    "                img = Image.open(data_dir + file)\n",
    "                self.subjects[subject].add(np.array(img, dtype=np.float64).reshape(-1, ), label) # Reshaped to a vector        \n",
    "        \n",
    "    def build_dataset(self, test_subject_id, labels):\n",
    "        train, holdout, test, pca = Dataset(), Dataset(), Dataset(), []\n",
    "        \n",
    "        # Select data for train, holdout and test dataset\n",
    "        subject_ids = self.get_subject_ids()\n",
    "        test_subject = self.subjects[test_subject_id]\n",
    "        subject_ids.remove(test_subject_id)\n",
    "        \n",
    "        holdout_subject_id = random.choice(subject_ids)\n",
    "        holdout_subject = self.subjects[holdout_subject_id]\n",
    "        subject_ids.remove(holdout_subject_id)\n",
    "        \n",
    "        for label in labels:\n",
    "            test.insert(test_subject.get(label), label)\n",
    "            holdout.insert(holdout_subject.get(label), label)\n",
    "            train.extend([self.subjects[train_subject_id].get(label) for train_subject_id in subject_ids], [label] * len(subject_ids))\n",
    "            \n",
    "        # Select data for PCA\n",
    "        for train_subject_id in subject_ids:\n",
    "            pca.extend(list(self.subjects[train_subject_id].label_image_dict.values()))\n",
    "        \n",
    "        # To numpy array\n",
    "        train.to_numpy_array()\n",
    "        holdout.to_numpy_array()\n",
    "        test.to_numpy_array()\n",
    "        pca = np.array(pca)\n",
    "        \n",
    "        # Normalizatiton\n",
    "        mean = np.mean(pca, axis=0)\n",
    "        pca_normalized = (pca - mean)\n",
    "        train.data = (train.data - mean) \n",
    "        holdout.data = (holdout.data - mean)\n",
    "        test.data = (test.data - mean)\n",
    "        \n",
    "        return train, holdout, test, pca_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_face(img):\n",
    "    \"\"\" Display the input image and optionally save as a PNG.\n",
    "    Args:\n",
    "        img: The NumPy array or image to display\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    img = img.reshape(380, 240)\n",
    "    # Convert img to PIL Image object (if it's an ndarray)\n",
    "    if type(img) == np.ndarray:\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(np.asarray(img), cmap='gray') # for jupyter notebook inline display\n",
    "    plt.axis('off')\n",
    "        \n",
    "def display_faces(images, layout, labels):\n",
    "    (n_row, n_col) = layout\n",
    "    assert n_row*n_col == len(images) == len(labels)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)  # 1-6\n",
    "        plt.title(\"{} face\".format(labels[i]))\n",
    "        display_face(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PCA:\n",
    "    \n",
    "#     def __init__(self, data):\n",
    "#         \"\"\"\n",
    "#         data is stored row wise\n",
    "#         \"\"\"\n",
    "#         self.data = data\n",
    "#         self.eig_val, self.eig_vec = self.__pca__(data)\n",
    "    \n",
    "#     def __pca__(self, data):\n",
    "#         \"\"\"\n",
    "#         returns: data transformed in 2 dims/columns + regenerated original data\n",
    "#         pass in: data as 2D NumPy array\n",
    "#         \"\"\" \n",
    "        \n",
    "#         data -= data.mean(axis=0)\n",
    "\n",
    "#         # Calculate covariance\n",
    "#         cov = np.dot(data, data.T)\n",
    "\n",
    "#         eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        \n",
    "#         # Map the eigenvectors to original ones\n",
    "#         eig_vecs = np.dot(data.T, eig_vecs)\n",
    "        \n",
    "#         # Normalization\n",
    "#         eig_vecs = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "# #         print(np.dot(data, eig_vecs[:,1:])/np.sqrt(eig_vals[1:]))\n",
    "# #         print(np.var(np.dot(data, eig_vecs)))\n",
    "# #         print(eig_vals)\n",
    "#         return eig_vals[1:], eig_vecs[:, 1:]\n",
    "    \n",
    "#     def transform(self, data, n_components):\n",
    "# #         selected_idx = self.sorted_idx[:n_components]\n",
    "#         tran_data = np.dot(data, self.eig_vec[:, -n_components:])\n",
    "# #         print(\"tran\")\n",
    "# #         print(tran_data / np.sqrt(self.eig_val[-n_components:]))\n",
    "#         return tran_data / np.sqrt(self.eig_val[-n_components:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22222222 0.66666667 1.55555556 0.66666667]\n",
      "[1.45738792e-15 2.90283246e+00 6.43050087e+00]\n",
      "[-1.28255434e-16  4.99486258e-16  2.90283246e+00  6.43050087e+00]\n",
      "[[-0.84287326  0.42710867  0.07953533 -0.31751691]\n",
      " [-0.22198016 -0.52168322  0.8185381   0.09255701]\n",
      " [ 0.04423319 -0.49803958 -0.21043072 -0.84007078]\n",
      " [ 0.48819351  0.54532685  0.52857205 -0.42999686]]\n",
      "[[ 0.07953533 -0.31751691]\n",
      " [ 0.8185381   0.09255701]\n",
      " [-0.21043072 -0.84007078]\n",
      " [ 0.52857205 -0.42999686]]\n",
      "[[2.90283246e+00 2.57734637e-16]\n",
      " [2.57734637e-16 6.43050087e+00]]\n",
      "[0.96761082 2.14350029]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,4,5],[2,3,6,7],[1,4,3,6]])\n",
    "print(np.var(x, axis=0))\n",
    "y = x-x.mean(axis=0)\n",
    "z  = np.dot(y, y.T)\n",
    "v, w = np.linalg.eigh(z)\n",
    "print(v)\n",
    "u = np.dot(y.T,y)\n",
    "g,h =  np.linalg.eigh(u)\n",
    "print(g)\n",
    "# print(np.diag(u))\n",
    "print(h)\n",
    "r = np.dot(y.T, w)\n",
    "r = r / np.linalg.norm(r, 2, axis=0)\n",
    "r = r[:,1:]\n",
    "print(r)\n",
    "new_y = np.dot(y, r)\n",
    "print(np.dot(new_y.T, new_y))\n",
    "print(np.var(new_y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def learn(self, raw):\n",
    "        # Subtract the mean\n",
    "        self.mu = raw.mean(axis=0)\n",
    "        raw -= self.mu\n",
    "\n",
    "#         # Calculate covariance\n",
    "        cov = np.dot(raw, raw.T)\n",
    "        eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        self.std = np.sqrt(eig_vals)[1:]\n",
    "\n",
    "        # Map the eigenvectors to original ones\n",
    "        eig_vecs = np.dot(raw.T, eig_vecs)[:, 1:]\n",
    "\n",
    "        # Normalization\n",
    "        self.transformer = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "        \n",
    "    def run(self, data, n_components, normalize=True):\n",
    "#         data = data - self.mu\n",
    "        data = np.dot(data, self.transformer[:, -n_components:])\n",
    "        if normalize:\n",
    "            data = data / self.std[-n_components:]\n",
    "        return data\n",
    "    \n",
    "    def plt_eig_faces(self, n_faces=6, layout=(2,3)):\n",
    "        # display eigenfaces\n",
    "        display_faces(self.transformer.T[0:n_faces], layout=layout, \n",
    "                      labels=[\"eigenface {}\".format(i) for i in range(n_faces)])\n",
    "        plt.savefig('Eigenfaces.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display 6 different emotional faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1(c) Display eigen face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.train_set, self.holdout_set, self.test_set = None, None, None\n",
    "        self.w = None\n",
    "        \n",
    "        self.test_accuracies = []\n",
    "    \n",
    "    def load_data(self, train, holdout, test):\n",
    "        self.train_set, self.holdout_set, self.test_set = train, holdout, test\n",
    "        \n",
    "        label_set = list(set(self.train_set.labels))\n",
    "        assert len(label_set) == 2\n",
    "        \n",
    "        self.encode = self.encoder(train.labels)\n",
    "        \n",
    "        for dataset in [self.train_set, self.holdout_set, self.test_set]:\n",
    "            dataset.X = self.bias(dataset.data)\n",
    "            dataset.y = self.encode(dataset.labels)\n",
    "        \n",
    "        self.test_set.y = np.array([True if (l - 1.0) < 1e-10 else False for l in self.test_set.y])\n",
    "        \n",
    "    def bias(self, data):\n",
    "        return np.column_stack((np.ones(len(data)), data))\n",
    "\n",
    "    def accuracy(self, test_set=None):\n",
    "        if not test_set:\n",
    "            test_set = self.test_set\n",
    "#         y = np.argmax(y, axis=1)[:, np.newaxis]\n",
    "        return np.sum(self.predict(test_set.X) == test_set.y) / len(test_set.y)\n",
    "    \n",
    "    def test(self):\n",
    "        self.test_accuracies.append(self.accuracy())\n",
    "        \n",
    "    def train(self, T=10, lr=0.06, bs=None):\n",
    "        \n",
    "        # initialization\n",
    "        train_X, train_y = self.train_set.X, self.train_set.y\n",
    "        holdout_X, holdout_y = self.holdout_set.X, self.holdout_set.y\n",
    "        self.train_losses, self.holdout_losses = [], []\n",
    "        self.w = np.zeros((train_X.shape[1], train_y.shape[1]))\n",
    "        self.W = []\n",
    "#         self.W = np.array([])\n",
    "\n",
    "        # gradient descent\n",
    "        for t in range(T):\n",
    "            # gradient descent on each batch\n",
    "            if bs:\n",
    "                # generate random permutation\n",
    "                perm = np.random.permutation(len(train_X))\n",
    "                for i in range(round(len(train_X)/bs)):\n",
    "                    train_X_batch, train_y_batch = train_X[perm[i:i+bs]], train_y[perm[i:i+bs]]\n",
    "                    self.w -= lr * self.gradient(train_X_batch, train_y_batch)\n",
    "            else:\n",
    "                self.w -= lr * self.gradient(train_X, train_y)\n",
    "#                 print(\"temporary w: {}\".format(self.w))\n",
    "            self.W.append(self.w.tolist())\n",
    "#             self.W = np.append(self.W, self.w)\n",
    "#             print(\"temporary W: {}\".format(self.W))\n",
    "            \n",
    "            # compute losses on train dataset and holdout dataset\n",
    "            print(\"train losses\")\n",
    "            print(self.loss(train_X, train_y))\n",
    "            self.train_losses.append(self.loss(train_X, train_y))\n",
    "            self.holdout_losses.append(self.loss(holdout_X, holdout_y))\n",
    "            \n",
    "        # save the parameters with best performance\n",
    "#         print(\"The W: {}\".format(np.array(self.W)))\n",
    "        self.w = np.array(min(self.W, key=lambda w: self.holdout_losses[self.W.index(w)]))\n",
    "#         print(self.holdout_losses)\n",
    "#         print(self.W)\n",
    "#         print(np.array(self.W).shape)\n",
    "#         print(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        assert len(label_set) == 2\n",
    "        binary_dict = {label_set[0]:0, label_set[1]:1}\n",
    "        return lambda labels: np.array([binary_dict[label] for label in labels]).reshape(-1, 1)\n",
    "    \n",
    "    def logistic(self, s):\n",
    "        return np.array(1 / (1 + np.exp(-s)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.logistic(np.dot(X, self.w))\n",
    "        return np.array([1 for prob in probs if prob>0.5])\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        print(\"logistic losses: {}\".format(- np.dot(y.T, np.log(y_hat)) / len(y_hat)))\n",
    "#         return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "        return (- np.dot(y.T, np.log(y_hat)) / len(y_hat))[0][0]\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        return np.sum((y_hat - y) * X, axis=0).reshape(-1, 1) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        one_hot_dict = {label:one_hot for label, one_hot in zip(label_set, np.eye(len(label_set)))}\n",
    "        return lambda labels: np.array([one_hot_dict[label] for label in labels])\n",
    "    \n",
    "    def softmax(self, s):\n",
    "        return np.exp(s) / np.sum(np.exp(s), axis=1, keepdims=True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.softmax(np.dot(X, self.w))\n",
    "        return np.argmax(probs, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        print(\"softmax losses: {}\".format(- np.sum(y * np.log(y_hat)) / len(y_hat)))\n",
    "        return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        return np.dot(X.T, (y_hat - y)) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Records:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.holdout_losses = []\n",
    "        self.test_accuracies = []\n",
    "                \n",
    "    def record(self, train_los, holdout_los, test_acc):\n",
    "        self.train_losses.append(train_los)\n",
    "        self.holdout_losses.append(holdout_los)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        \n",
    "    def plt_losses(self, n_components, lr, n_epoches, train=True, holdout=True):\n",
    "        assert len(self.train_losses) == len(self.holdout_losses)\n",
    "        plt.figure()\n",
    "        if train:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.train_losses, axis=0), yerr=np.std(self.train_losses, axis=0))\n",
    "        if holdout:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.holdout_losses, axis=0), yerr=np.std(self.holdout_losses, axis=0))\n",
    "        plt.title(\"n_components={}, learning_rates={}, n_epoches={}\".format(n_components, lr, n_epoches))\n",
    "        plt.show()\n",
    "        \n",
    "    def show_accuracies(self):\n",
    "        print(\"The accuracy is {}\".format(np.mean(self.test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, facial_expressions, dataset_dir='./CAFE/', classifier_type=\"logistic\", pca=True):\n",
    "        self.facial_expressions = facial_expressions\n",
    "        \n",
    "        self.data_builder = DataBuilder()\n",
    "        self.data_builder.load_data(dataset_dir)\n",
    "        \n",
    "        if classifier_type==\"logistic\":\n",
    "            self.classifier = LogisticClassifier()\n",
    "        elif classifier_type==\"softmax\":\n",
    "            self.classifier = SoftmaxClassifier()\n",
    "            \n",
    "        if pca:\n",
    "            self.PCA = PCA()\n",
    "            \n",
    "        self.records = Records()\n",
    "        \n",
    "    def build(self, n_components, learning_rate, n_epoches, batch_size=None, n_repeats=10):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoches = n_epoches\n",
    "        self.batch_size = batch_size\n",
    "        self.n_repeats = n_repeats\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        subjects = self.data_builder.get_subject_ids()\n",
    "        print(\"subject: {}\".format(subjects))\n",
    "        for repeat in range(self.n_repeats):\n",
    "            for test_subject in subjects:\n",
    "                # Dataset building\n",
    "                train, holdout, test, pca_data = self.data_builder.build_dataset(test_subject, self.facial_expressions)\n",
    "                self.PCA.learn(pca_data)\n",
    "                \n",
    "                train.data = self.PCA.run(train.data, self.n_components)\n",
    "                holdout.data = self.PCA.run(holdout.data, self.n_components)\n",
    "                test.data = self.PCA.run(test.data, self.n_components)\n",
    "\n",
    "                # Run classification algorithm\n",
    "                self.classifier.load_data(train, holdout, test)\n",
    "                self.classifier.train(lr=self.learning_rate, T=self.n_epoches, bs=self.batch_size)\n",
    "                self.classifier.test()\n",
    "                self.records.record(self.classifier.train_losses, self.classifier.holdout_losses, self.classifier.test_accuracies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: ['037', '041', '044', '043', '018', '048ng', '036', '049', '027', '050']\n",
      "train losses\n",
      "logistic losses: [[0.34615879]]\n",
      "0.34615878819895723\n",
      "logistic losses: [[0.34615879]]\n",
      "logistic losses: [[0.34621583]]\n",
      "train losses\n",
      "logistic losses: [[0.34574594]]\n",
      "0.34574593726560293\n",
      "logistic losses: [[0.34574594]]\n",
      "logistic losses: [[0.34585984]]\n",
      "train losses\n",
      "logistic losses: [[0.34533501]]\n",
      "0.345335013335371\n",
      "logistic losses: [[0.34533501]]\n",
      "logistic losses: [[0.34550559]]\n",
      "train losses\n",
      "logistic losses: [[0.34492599]]\n",
      "0.3449259926425798\n",
      "logistic losses: [[0.34492599]]\n",
      "logistic losses: [[0.34515306]]\n",
      "train losses\n",
      "logistic losses: [[0.34451885]]\n",
      "0.34451885179418834\n",
      "logistic losses: [[0.34451885]]\n",
      "logistic losses: [[0.34480222]]\n",
      "train losses\n",
      "logistic losses: [[0.34411357]]\n",
      "0.3441135677636844\n",
      "logistic losses: [[0.34411357]]\n",
      "logistic losses: [[0.34445305]]\n",
      "train losses\n",
      "logistic losses: [[0.34371012]]\n",
      "0.34371011788507644\n",
      "logistic losses: [[0.34371012]]\n",
      "logistic losses: [[0.34410554]]\n",
      "train losses\n",
      "logistic losses: [[0.34330848]]\n",
      "0.34330847984698654\n",
      "logistic losses: [[0.34330848]]\n",
      "logistic losses: [[0.34375965]]\n",
      "train losses\n",
      "logistic losses: [[0.34290863]]\n",
      "0.34290863168684305\n",
      "logistic losses: [[0.34290863]]\n",
      "logistic losses: [[0.34341537]]\n",
      "train losses\n",
      "logistic losses: [[0.34251055]]\n",
      "0.3425105517851709\n",
      "logistic losses: [[0.34251055]]\n",
      "logistic losses: [[0.34307267]]\n",
      "train losses\n",
      "logistic losses: [[0.34615791]]\n",
      "0.34615791413746194\n",
      "logistic losses: [[0.34615791]]\n",
      "logistic losses: [[0.34640729]]\n",
      "train losses\n",
      "logistic losses: [[0.34574411]]\n",
      "0.345744109167893\n",
      "logistic losses: [[0.34574411]]\n",
      "logistic losses: [[0.34624234]]\n",
      "train losses\n",
      "logistic losses: [[0.34533215]]\n",
      "0.34533215261875605\n",
      "logistic losses: [[0.34533215]]\n",
      "logistic losses: [[0.34607872]]\n",
      "train losses\n",
      "logistic losses: [[0.34492202]]\n",
      "0.34492202209418493\n",
      "logistic losses: [[0.34492202]]\n",
      "logistic losses: [[0.34591641]]\n",
      "train losses\n",
      "logistic losses: [[0.3445137]]\n",
      "0.3445136955491134\n",
      "logistic losses: [[0.3445137]]\n",
      "logistic losses: [[0.34575538]]\n",
      "train losses\n",
      "logistic losses: [[0.34410715]]\n",
      "0.3441071512835308\n",
      "logistic losses: [[0.34410715]]\n",
      "logistic losses: [[0.34559562]]\n",
      "train losses\n",
      "logistic losses: [[0.34370237]]\n",
      "0.34370236793683406\n",
      "logistic losses: [[0.34370237]]\n",
      "logistic losses: [[0.34543711]]\n",
      "train losses\n",
      "logistic losses: [[0.34329932]]\n",
      "0.3432993244822742\n",
      "logistic losses: [[0.34329932]]\n",
      "logistic losses: [[0.34527983]]\n",
      "train losses\n",
      "logistic losses: [[0.342898]]\n",
      "0.3428980002214962\n",
      "logistic losses: [[0.342898]]\n",
      "logistic losses: [[0.34512376]]\n",
      "train losses\n",
      "logistic losses: [[0.34249837]]\n",
      "0.3424983747791708\n",
      "logistic losses: [[0.34249837]]\n",
      "logistic losses: [[0.34496889]]\n",
      "train losses\n",
      "logistic losses: [[0.34617743]]\n",
      "0.3461774339444488\n",
      "logistic losses: [[0.34617743]]\n",
      "logistic losses: [[0.34622229]]\n",
      "train losses\n",
      "logistic losses: [[0.34578285]]\n",
      "0.3457828487379813\n",
      "logistic losses: [[0.34578285]]\n",
      "logistic losses: [[0.34587244]]\n",
      "train losses\n",
      "logistic losses: [[0.34538982]]\n",
      "0.34538981679473546\n",
      "logistic losses: [[0.34538982]]\n",
      "logistic losses: [[0.34552402]]\n",
      "train losses\n",
      "logistic losses: [[0.34499832]]\n",
      "0.3449983205241257\n",
      "logistic losses: [[0.34499832]]\n",
      "logistic losses: [[0.34517701]]\n",
      "train losses\n",
      "logistic losses: [[0.34460834]]\n",
      "0.34460834260636086\n",
      "logistic losses: [[0.34460834]]\n",
      "logistic losses: [[0.3448314]]\n",
      "train losses\n",
      "logistic losses: [[0.34421987]]\n",
      "0.3442198659880626\n",
      "logistic losses: [[0.34421987]]\n",
      "logistic losses: [[0.34448717]]\n",
      "train losses\n",
      "logistic losses: [[0.34383287]]\n",
      "0.34383287387795675\n",
      "logistic losses: [[0.34383287]]\n",
      "logistic losses: [[0.34414431]]\n",
      "train losses\n",
      "logistic losses: [[0.34344735]]\n",
      "0.34344734974263535\n",
      "logistic losses: [[0.34344735]]\n",
      "logistic losses: [[0.34380279]]\n",
      "train losses\n",
      "logistic losses: [[0.34306328]]\n",
      "0.34306327730238884\n",
      "logistic losses: [[0.34306328]]\n",
      "logistic losses: [[0.34346261]]\n",
      "train losses\n",
      "logistic losses: [[0.34268064]]\n",
      "0.3426806405271065\n",
      "logistic losses: [[0.34268064]]\n",
      "logistic losses: [[0.34312374]]\n",
      "train losses\n",
      "logistic losses: [[0.34616468]]\n",
      "0.34616468449736104\n",
      "logistic losses: [[0.34616468]]\n",
      "logistic losses: [[0.34597651]]\n",
      "train losses\n",
      "logistic losses: [[0.34575768]]\n",
      "0.34575767797595036\n",
      "logistic losses: [[0.34575768]]\n",
      "logistic losses: [[0.34538185]]\n",
      "train losses\n",
      "logistic losses: [[0.34535255]]\n",
      "0.34535254642704977\n",
      "logistic losses: [[0.34535255]]\n",
      "logistic losses: [[0.34478957]]\n",
      "train losses\n",
      "logistic losses: [[0.34494927]]\n",
      "0.3449492659480703\n",
      "logistic losses: [[0.34494927]]\n",
      "logistic losses: [[0.34419967]]\n",
      "train losses\n",
      "logistic losses: [[0.34454781]]\n",
      "0.3445478130161471\n",
      "logistic losses: [[0.34454781]]\n",
      "logistic losses: [[0.3436121]]\n",
      "train losses\n",
      "logistic losses: [[0.34414816]]\n",
      "0.3441481644818696\n",
      "logistic losses: [[0.34414816]]\n",
      "logistic losses: [[0.34302685]]\n",
      "train losses\n",
      "logistic losses: [[0.3437503]]\n",
      "0.3437502975631189\n",
      "logistic losses: [[0.3437503]]\n",
      "logistic losses: [[0.34244388]]\n",
      "train losses\n",
      "logistic losses: [[0.34335419]]\n",
      "0.34335418983901034\n",
      "logistic losses: [[0.34335419]]\n",
      "logistic losses: [[0.34186318]]\n",
      "train losses\n",
      "logistic losses: [[0.34295982]]\n",
      "0.34295981924393815\n",
      "logistic losses: [[0.34295982]]\n",
      "logistic losses: [[0.34128472]]\n",
      "train losses\n",
      "logistic losses: [[0.34256716]]\n",
      "0.34256716406172205\n",
      "logistic losses: [[0.34256716]]\n",
      "logistic losses: [[0.34070847]]\n",
      "train losses\n",
      "logistic losses: [[0.34615147]]\n",
      "0.3461514675509819\n",
      "logistic losses: [[0.34615147]]\n",
      "logistic losses: [[0.34618035]]\n",
      "train losses\n",
      "logistic losses: [[0.34573132]]\n",
      "0.3457313170925461\n",
      "logistic losses: [[0.34573132]]\n",
      "logistic losses: [[0.34578898]]\n",
      "train losses\n",
      "logistic losses: [[0.34531311]]\n",
      "0.34531311417325417\n",
      "logistic losses: [[0.34531311]]\n",
      "logistic losses: [[0.34539946]]\n",
      "train losses\n",
      "logistic losses: [[0.34489683]]\n",
      "0.3448968344531238\n",
      "logistic losses: [[0.34489683]]\n",
      "logistic losses: [[0.34501175]]\n",
      "train losses\n",
      "logistic losses: [[0.34448245]]\n",
      "0.3444824539771405\n",
      "logistic losses: [[0.34448245]]\n",
      "logistic losses: [[0.34462584]]\n",
      "train losses\n",
      "logistic losses: [[0.34406995]]\n",
      "0.34406994916890704\n",
      "logistic losses: [[0.34406995]]\n",
      "logistic losses: [[0.3442417]]\n",
      "train losses\n",
      "logistic losses: [[0.3436593]]\n",
      "0.3436592968244001\n",
      "logistic losses: [[0.3436593]]\n",
      "logistic losses: [[0.34385931]]\n",
      "train losses\n",
      "logistic losses: [[0.34325047]]\n",
      "0.3432504741058333\n",
      "logistic losses: [[0.34325047]]\n",
      "logistic losses: [[0.34347866]]\n",
      "train losses\n",
      "logistic losses: [[0.34284346]]\n",
      "0.3428434585356248\n",
      "logistic losses: [[0.34284346]]\n",
      "logistic losses: [[0.34309971]]\n",
      "train losses\n",
      "logistic losses: [[0.34243823]]\n",
      "0.34243822799046564\n",
      "logistic losses: [[0.34243823]]\n",
      "logistic losses: [[0.34272244]]\n",
      "train losses\n",
      "logistic losses: [[0.34616468]]\n",
      "0.34616468449736104\n",
      "logistic losses: [[0.34616468]]\n",
      "logistic losses: [[0.34612385]]\n",
      "train losses\n",
      "logistic losses: [[0.34575768]]\n",
      "0.34575767797595036\n",
      "logistic losses: [[0.34575768]]\n",
      "logistic losses: [[0.34567611]]\n",
      "train losses\n",
      "logistic losses: [[0.34535255]]\n",
      "0.34535254642704977\n",
      "logistic losses: [[0.34535255]]\n",
      "logistic losses: [[0.34523035]]\n",
      "train losses\n",
      "logistic losses: [[0.34494927]]\n",
      "0.3449492659480703\n",
      "logistic losses: [[0.34494927]]\n",
      "logistic losses: [[0.34478655]]\n",
      "train losses\n",
      "logistic losses: [[0.34454781]]\n",
      "0.3445478130161471\n",
      "logistic losses: [[0.34454781]]\n",
      "logistic losses: [[0.34434468]]\n",
      "train losses\n",
      "logistic losses: [[0.34414816]]\n",
      "0.3441481644818696\n",
      "logistic losses: [[0.34414816]]\n",
      "logistic losses: [[0.34390471]]\n",
      "train losses\n",
      "logistic losses: [[0.3437503]]\n",
      "0.3437502975631189\n",
      "logistic losses: [[0.3437503]]\n",
      "logistic losses: [[0.34346663]]\n",
      "train losses\n",
      "logistic losses: [[0.34335419]]\n",
      "0.34335418983901034\n",
      "logistic losses: [[0.34335419]]\n",
      "logistic losses: [[0.34303041]]\n",
      "train losses\n",
      "logistic losses: [[0.34295982]]\n",
      "0.34295981924393815\n",
      "logistic losses: [[0.34295982]]\n",
      "logistic losses: [[0.34259603]]\n",
      "train losses\n",
      "logistic losses: [[0.34256716]]\n",
      "0.34256716406172205\n",
      "logistic losses: [[0.34256716]]\n",
      "logistic losses: [[0.34216346]]\n",
      "train losses\n",
      "logistic losses: [[0.34614403]]\n",
      "0.3461440328415535\n",
      "logistic losses: [[0.34614403]]\n",
      "logistic losses: [[0.34611131]]\n",
      "train losses\n",
      "logistic losses: [[0.34571667]]\n",
      "0.3457166667216739\n",
      "logistic losses: [[0.34571667]]\n",
      "logistic losses: [[0.34565128]]\n",
      "train losses\n",
      "logistic losses: [[0.34529146]]\n",
      "0.34529146344950223\n",
      "logistic losses: [[0.34529146]]\n",
      "logistic losses: [[0.34519344]]\n",
      "train losses\n",
      "logistic losses: [[0.3448684]]\n",
      "0.34486839500691413\n",
      "logistic losses: [[0.3448684]]\n",
      "logistic losses: [[0.34473779]]\n",
      "train losses\n",
      "logistic losses: [[0.34444743]]\n",
      "0.34444743382098386\n",
      "logistic losses: [[0.34444743]]\n",
      "logistic losses: [[0.34428429]]\n",
      "train losses\n",
      "logistic losses: [[0.34402855]]\n",
      "0.34402855275660427\n",
      "logistic losses: [[0.34402855]]\n",
      "logistic losses: [[0.34383292]]\n",
      "train losses\n",
      "logistic losses: [[0.34361173]]\n",
      "0.3436117251092339\n",
      "logistic losses: [[0.34361173]]\n",
      "logistic losses: [[0.34338364]]\n",
      "train losses\n",
      "logistic losses: [[0.34319692]]\n",
      "0.34319692459776857\n",
      "logistic losses: [[0.34319692]]\n",
      "logistic losses: [[0.34293643]]\n",
      "train losses\n",
      "logistic losses: [[0.34278413]]\n",
      "0.34278412535753433\n",
      "logistic losses: [[0.34278413]]\n",
      "logistic losses: [[0.34249127]]\n",
      "train losses\n",
      "logistic losses: [[0.3423733]]\n",
      "0.3423733019334013\n",
      "logistic losses: [[0.3423733]]\n",
      "logistic losses: [[0.34204812]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train losses\n",
      "logistic losses: [[0.34614478]]\n",
      "0.34614477852992753\n",
      "logistic losses: [[0.34614478]]\n",
      "logistic losses: [[0.34618404]]\n",
      "train losses\n",
      "logistic losses: [[0.34571815]]\n",
      "0.34571814850883853\n",
      "logistic losses: [[0.34571815]]\n",
      "logistic losses: [[0.34579658]]\n",
      "train losses\n",
      "logistic losses: [[0.34529367]]\n",
      "0.34529367140038847\n",
      "logistic losses: [[0.34529367]]\n",
      "logistic losses: [[0.34541116]]\n",
      "train losses\n",
      "logistic losses: [[0.34487132]]\n",
      "0.3448713188503963\n",
      "logistic losses: [[0.34487132]]\n",
      "logistic losses: [[0.34502777]]\n",
      "train losses\n",
      "logistic losses: [[0.34445106]]\n",
      "0.34445106295910505\n",
      "logistic losses: [[0.34445106]]\n",
      "logistic losses: [[0.34464638]]\n",
      "train losses\n",
      "logistic losses: [[0.34403288]]\n",
      "0.3440328762736046\n",
      "logistic losses: [[0.34403288]]\n",
      "logistic losses: [[0.34426696]]\n",
      "train losses\n",
      "logistic losses: [[0.34361673]]\n",
      "0.3436167317803847\n",
      "logistic losses: [[0.34361673]]\n",
      "logistic losses: [[0.34388949]]\n",
      "train losses\n",
      "logistic losses: [[0.3432026]]\n",
      "0.34320260289801674\n",
      "logistic losses: [[0.3432026]]\n",
      "logistic losses: [[0.34351393]]\n",
      "train losses\n",
      "logistic losses: [[0.34279046]]\n",
      "0.3427904634699619\n",
      "logistic losses: [[0.34279046]]\n",
      "logistic losses: [[0.34314027]]\n",
      "train losses\n",
      "logistic losses: [[0.34238029]]\n",
      "0.34238028775750284\n",
      "logistic losses: [[0.34238029]]\n",
      "logistic losses: [[0.34276847]]\n",
      "train losses\n",
      "logistic losses: [[0.34617926]]\n",
      "0.3461792642048469\n",
      "logistic losses: [[0.34617926]]\n",
      "logistic losses: [[0.34625035]]\n",
      "train losses\n",
      "logistic losses: [[0.34578685]]\n",
      "0.3457868493031284\n",
      "logistic losses: [[0.34578685]]\n",
      "logistic losses: [[0.34592891]]\n",
      "train losses\n",
      "logistic losses: [[0.34539632]]\n",
      "0.34539632064872394\n",
      "logistic losses: [[0.34539632]]\n",
      "logistic losses: [[0.34560922]]\n",
      "train losses\n",
      "logistic losses: [[0.34500765]]\n",
      "0.34500765371269626\n",
      "logistic losses: [[0.34500765]]\n",
      "logistic losses: [[0.34529127]]\n",
      "train losses\n",
      "logistic losses: [[0.34462082]]\n",
      "0.3446208243566889\n",
      "logistic losses: [[0.34462082]]\n",
      "logistic losses: [[0.34497504]]\n",
      "train losses\n",
      "logistic losses: [[0.34423581]]\n",
      "0.3442358088264631\n",
      "logistic losses: [[0.34423581]]\n",
      "logistic losses: [[0.34466049]]\n",
      "train losses\n",
      "logistic losses: [[0.34385258]]\n",
      "0.34385258374554584\n",
      "logistic losses: [[0.34385258]]\n",
      "logistic losses: [[0.34434762]]\n",
      "train losses\n",
      "logistic losses: [[0.34347113]]\n",
      "0.3434711261089863\n",
      "logistic losses: [[0.34347113]]\n",
      "logistic losses: [[0.34403639]]\n",
      "train losses\n",
      "logistic losses: [[0.34309141]]\n",
      "0.3430914132772189\n",
      "logistic losses: [[0.34309141]]\n",
      "logistic losses: [[0.34372679]]\n",
      "train losses\n",
      "logistic losses: [[0.34271342]]\n",
      "0.34271342297003143\n",
      "logistic losses: [[0.34271342]]\n",
      "logistic losses: [[0.34341879]]\n",
      "train losses\n",
      "logistic losses: [[0.34616052]]\n",
      "0.34616052408051007\n",
      "logistic losses: [[0.34616052]]\n",
      "logistic losses: [[0.34636128]]\n",
      "train losses\n",
      "logistic losses: [[0.3457496]]\n",
      "0.34574960399118626\n",
      "logistic losses: [[0.3457496]]\n",
      "logistic losses: [[0.34615069]]\n",
      "train losses\n",
      "logistic losses: [[0.3453408]]\n",
      "0.3453408012681504\n",
      "logistic losses: [[0.3453408]]\n",
      "logistic losses: [[0.34594178]]\n",
      "train losses\n",
      "logistic losses: [[0.34493409]]\n",
      "0.3449340876284116\n",
      "logistic losses: [[0.34493409]]\n",
      "logistic losses: [[0.34573453]]\n",
      "train losses\n",
      "logistic losses: [[0.34452944]]\n",
      "0.3445294352421553\n",
      "logistic losses: [[0.34452944]]\n",
      "logistic losses: [[0.34552892]]\n",
      "train losses\n",
      "logistic losses: [[0.34412682]]\n",
      "0.3441268167251929\n",
      "logistic losses: [[0.34412682]]\n",
      "logistic losses: [[0.34532493]]\n",
      "train losses\n",
      "logistic losses: [[0.34372621]]\n",
      "0.34372620513154206\n",
      "logistic losses: [[0.34372621]]\n",
      "logistic losses: [[0.34512252]]\n",
      "train losses\n",
      "logistic losses: [[0.34332757]]\n",
      "0.3433275739461354\n",
      "logistic losses: [[0.34332757]]\n",
      "logistic losses: [[0.34492168]]\n",
      "train losses\n",
      "logistic losses: [[0.3429309]]\n",
      "0.34293089707765456\n",
      "logistic losses: [[0.3429309]]\n",
      "logistic losses: [[0.34472238]]\n",
      "train losses\n",
      "logistic losses: [[0.34253615]]\n",
      "0.3425361488514891\n",
      "logistic losses: [[0.34253615]]\n",
      "logistic losses: [[0.3445246]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17c32aa96d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "# n_components = [20, 30, 40]\n",
    "# learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05] # 10] # 0.01 - 1\n",
    "\n",
    "n_components = [20]\n",
    "learning_rates = [0.06]\n",
    "n_epoches = 10\n",
    "for n_comp in n_components:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "#         pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"softmax\")\n",
    "        pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"logistic\")\n",
    "        pipeline.build(n_components=n_comp, learning_rate=lr, n_epoches=n_epoches, batch_size=None, n_repeats=1)\n",
    "        pipeline.run()\n",
    "        \n",
    "        # plot errors\n",
    "        pipeline.records.plt_losses(n_components=n_comp, lr=lr, n_epoches=n_epoches)\n",
    "        pipeline.records.show_accuracies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
