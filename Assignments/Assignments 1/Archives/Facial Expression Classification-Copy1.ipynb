{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure code for dataset building  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_image_dict= {}\n",
    "    \n",
    "    def add(self, image, label):\n",
    "        self.label_image_dict[label] = image\n",
    "    \n",
    "    def get(self, label):\n",
    "        return self.label_image_dict[label]\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, data=[], labels=[]):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def to_numpy_array(self):\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "    def insert(self, datum, label):\n",
    "        self.data.append(datum)\n",
    "        self.labels.append(label)\n",
    "    \n",
    "    def extend(self, data, labels):\n",
    "        self.data.extend(data)\n",
    "        self.labels.extend(labels)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        idx = np.array(list(range(len(self.data))))\n",
    "        np.random.shuffle(idx)\n",
    "        self.data[:] = self.data[idx]\n",
    "        self.labels[:] = self.labels[idx]\n",
    "    \n",
    "class DataBuilder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.subjects = defaultdict(Subject)\n",
    "    \n",
    "    def get_subject_ids(self):\n",
    "        return list(self.subjects.keys())\n",
    "    \n",
    "    def load_data(self, data_dir=\"./CAFE/\"):\n",
    "        # Get the list of image file names\n",
    "        all_files = listdir(data_dir)\n",
    "        \n",
    "        # Store the images and labels in self.subjects dictionary\n",
    "        for file in all_files:\n",
    "            # Load in the files as PIL images and convert to NumPy arrays\n",
    "            subject, rest_string = file.split('_')\n",
    "            label = rest_string.split('.')[0][:-1]\n",
    "            \n",
    "            # Exclude neutral and happy faces \n",
    "            if label != 'n' and label != 'h':\n",
    "                img = Image.open(data_dir + file)\n",
    "                self.subjects[subject].add(np.array(img, dtype=np.float64).reshape(-1, ), label) # Reshaped to a vector        \n",
    "        \n",
    "    def build_dataset(self, test_subject_id, labels):\n",
    "        train, holdout, test, pca = Dataset(), Dataset(), Dataset(), []\n",
    "        \n",
    "        # Select data for train, holdout and test dataset\n",
    "        subject_ids = self.get_subject_ids()\n",
    "        test_subject = self.subjects[test_subject_id]\n",
    "        subject_ids.remove(test_subject_id)\n",
    "        \n",
    "        holdout_subject_id = random.choice(subject_ids)\n",
    "        holdout_subject = self.subjects[holdout_subject_id]\n",
    "        subject_ids.remove(holdout_subject_id)\n",
    "        \n",
    "        for label in labels:\n",
    "            test.insert(test_subject.get(label), label)\n",
    "            holdout.insert(holdout_subject.get(label), label)\n",
    "            train.extend([self.subjects[train_subject_id].get(label) for train_subject_id in subject_ids], [label] * len(subject_ids))\n",
    "            \n",
    "        # Select data for PCA\n",
    "        for train_subject_id in subject_ids:\n",
    "            pca.extend(list(self.subjects[train_subject_id].label_image_dict.values()))\n",
    "        \n",
    "        # To numpy array\n",
    "        train.to_numpy_array()\n",
    "        holdout.to_numpy_array()\n",
    "        test.to_numpy_array()\n",
    "        pca = np.array(pca)\n",
    "        \n",
    "        # Normalizatiton\n",
    "        mean = np.mean(pca, axis=0)\n",
    "        pca_normalized = (pca - mean)\n",
    "        train.data = (train.data - mean) \n",
    "        holdout.data = (holdout.data - mean)\n",
    "        test.data = (test.data - mean)\n",
    "        \n",
    "        return train, holdout, test, pca_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_face(img):\n",
    "    \"\"\" Display the input image and optionally save as a PNG.\n",
    "    Args:\n",
    "        img: The NumPy array or image to display\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    img = img.reshape(380, 240)\n",
    "    # Convert img to PIL Image object (if it's an ndarray)\n",
    "    if type(img) == np.ndarray:\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(np.asarray(img), cmap='gray') # for jupyter notebook inline display\n",
    "    plt.axis('off')\n",
    "        \n",
    "def display_faces(images, layout, labels):\n",
    "    (n_row, n_col) = layout\n",
    "    assert n_row*n_col == len(images) == len(labels)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)  # 1-6\n",
    "        plt.title(\"{} face\".format(labels[i]))\n",
    "        display_face(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PCA:\n",
    "    \n",
    "#     def __init__(self, data):\n",
    "#         \"\"\"\n",
    "#         data is stored row wise\n",
    "#         \"\"\"\n",
    "#         self.data = data\n",
    "#         self.eig_val, self.eig_vec = self.__pca__(data)\n",
    "    \n",
    "#     def __pca__(self, data):\n",
    "#         \"\"\"\n",
    "#         returns: data transformed in 2 dims/columns + regenerated original data\n",
    "#         pass in: data as 2D NumPy array\n",
    "#         \"\"\" \n",
    "        \n",
    "#         data -= data.mean(axis=0)\n",
    "\n",
    "#         # Calculate covariance\n",
    "#         cov = np.dot(data, data.T)\n",
    "\n",
    "#         eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        \n",
    "#         # Map the eigenvectors to original ones\n",
    "#         eig_vecs = np.dot(data.T, eig_vecs)\n",
    "        \n",
    "#         # Normalization\n",
    "#         eig_vecs = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "# #         print(np.dot(data, eig_vecs[:,1:])/np.sqrt(eig_vals[1:]))\n",
    "# #         print(np.var(np.dot(data, eig_vecs)))\n",
    "# #         print(eig_vals)\n",
    "#         return eig_vals[1:], eig_vecs[:, 1:]\n",
    "    \n",
    "#     def transform(self, data, n_components):\n",
    "# #         selected_idx = self.sorted_idx[:n_components]\n",
    "#         tran_data = np.dot(data, self.eig_vec[:, -n_components:])\n",
    "# #         print(\"tran\")\n",
    "# #         print(tran_data / np.sqrt(self.eig_val[-n_components:]))\n",
    "#         return tran_data / np.sqrt(self.eig_val[-n_components:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22222222 0.66666667 1.55555556 0.66666667]\n",
      "[1.45738792e-15 2.90283246e+00 6.43050087e+00]\n",
      "[-1.28255434e-16  4.99486258e-16  2.90283246e+00  6.43050087e+00]\n",
      "[[-0.84287326  0.42710867  0.07953533 -0.31751691]\n",
      " [-0.22198016 -0.52168322  0.8185381   0.09255701]\n",
      " [ 0.04423319 -0.49803958 -0.21043072 -0.84007078]\n",
      " [ 0.48819351  0.54532685  0.52857205 -0.42999686]]\n",
      "[[ 0.07953533 -0.31751691]\n",
      " [ 0.8185381   0.09255701]\n",
      " [-0.21043072 -0.84007078]\n",
      " [ 0.52857205 -0.42999686]]\n",
      "[[2.90283246e+00 2.57734637e-16]\n",
      " [2.57734637e-16 6.43050087e+00]]\n",
      "[0.96761082 2.14350029]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,4,5],[2,3,6,7],[1,4,3,6]])\n",
    "print(np.var(x, axis=0))\n",
    "y = x-x.mean(axis=0)\n",
    "z  = np.dot(y, y.T)\n",
    "v, w = np.linalg.eigh(z)\n",
    "print(v)\n",
    "u = np.dot(y.T,y)\n",
    "g,h =  np.linalg.eigh(u)\n",
    "print(g)\n",
    "# print(np.diag(u))\n",
    "print(h)\n",
    "r = np.dot(y.T, w)\n",
    "r = r / np.linalg.norm(r, 2, axis=0)\n",
    "r = r[:,1:]\n",
    "print(r)\n",
    "new_y = np.dot(y, r)\n",
    "print(np.dot(new_y.T, new_y))\n",
    "print(np.var(new_y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def learn(self, raw):\n",
    "        # Subtract the mean\n",
    "        self.mu = raw.mean(axis=0)\n",
    "        raw -= self.mu\n",
    "\n",
    "#         # Calculate covariance\n",
    "        cov = np.dot(raw, raw.T)\n",
    "        eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        self.std = np.sqrt(eig_vals)[1:]\n",
    "\n",
    "        # Map the eigenvectors to original ones\n",
    "        eig_vecs = np.dot(raw.T, eig_vecs)[:, 1:]\n",
    "\n",
    "        # Normalization\n",
    "        self.transformer = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "        \n",
    "    def run(self, data, n_components, normalize=True):\n",
    "#         data = data - self.mu\n",
    "        data = np.dot(data, self.transformer[:, -n_components:])\n",
    "        if normalize:\n",
    "            data = data / self.std[-n_components:]\n",
    "        return data\n",
    "    \n",
    "    def plt_eig_faces(self, n_faces=6, layout=(2,3)):\n",
    "        # display eigenfaces\n",
    "        display_faces(self.transformer.T[0:n_faces], layout=layout, \n",
    "                      labels=[\"eigenface {}\".format(i) for i in range(n_faces)])\n",
    "        plt.savefig('Eigenfaces.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display 6 different emotional faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1(c) Display eigen face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.train_set, self.holdout_set, self.test_set = None, None, None\n",
    "        self.w = None\n",
    "        \n",
    "        self.test_accuracies = []\n",
    "    \n",
    "    def load_data(self, train, holdout, test):\n",
    "        self.train_set, self.holdout_set, self.test_set = train, holdout, test\n",
    "        \n",
    "        label_set = list(set(self.train_set.labels))\n",
    "        assert len(label_set) == 2\n",
    "        \n",
    "        self.encode = self.encoder(train.labels)\n",
    "        \n",
    "        for dataset in [self.train_set, self.holdout_set, self.test_set]:\n",
    "            dataset.X = self.bias(dataset.data)\n",
    "            dataset.y = self.encode(dataset.labels)\n",
    "        \n",
    "        self.test_set.y = np.array([True if (l - 1.0) < 1e-10 else False for l in self.test_set.y])\n",
    "        \n",
    "    def bias(self, data):\n",
    "        return np.column_stack((np.ones(len(data)), data))\n",
    "\n",
    "    def accuracy(self, test_set=None):\n",
    "        if not test_set:\n",
    "            test_set = self.test_set\n",
    "#         y = np.argmax(y, axis=1)[:, np.newaxis]\n",
    "        return np.sum(self.predict(test_set.X) == test_set.y) / len(test_set.y)\n",
    "    \n",
    "    def test(self):\n",
    "        self.test_accuracies.append(self.accuracy())\n",
    "        \n",
    "    def train(self, T=10, lr=0.06, bs=None):\n",
    "        \n",
    "        # initialization\n",
    "        train_X, train_y = self.train_set.X, self.train_set.y\n",
    "        holdout_X, holdout_y = self.holdout_set.X, self.holdout_set.y\n",
    "        self.train_losses, self.holdout_losses = [], []\n",
    "        self.w = np.zeros((train_X.shape[1], train_y.shape[1]))\n",
    "        self.W = []\n",
    "#         self.W = np.array([])\n",
    "\n",
    "        # gradient descent\n",
    "        for t in range(T):\n",
    "            # gradient descent on each batch\n",
    "            if bs:\n",
    "                # generate random permutation\n",
    "                perm = np.random.permutation(len(train_X))\n",
    "                for i in range(round(len(train_X)/bs)):\n",
    "                    train_X_batch, train_y_batch = train_X[perm[i:i+bs]], train_y[perm[i:i+bs]]\n",
    "                    self.w -= lr * self.gradient(train_X_batch, train_y_batch)\n",
    "            else:\n",
    "                self.w -= lr * self.gradient(train_X, train_y)\n",
    "#                 print(\"temporary w: {}\".format(self.w))\n",
    "            self.W.append(self.w.tolist())\n",
    "#             self.W = np.append(self.W, self.w)\n",
    "#             print(\"temporary W: {}\".format(self.W))\n",
    "            \n",
    "            # compute losses on train dataset and holdout dataset\n",
    "            print(\"train losses\")\n",
    "            print(self.loss(train_X, train_y))\n",
    "            self.train_losses.append(self.loss(train_X, train_y))\n",
    "            self.holdout_losses.append(self.loss(holdout_X, holdout_y))\n",
    "            \n",
    "        # save the parameters with best performance\n",
    "#         print(\"The W: {}\".format(np.array(self.W)))\n",
    "        self.w = np.array(min(self.W, key=lambda w: self.holdout_losses[self.W.index(w)]))\n",
    "#         print(self.holdout_losses)\n",
    "#         print(self.W)\n",
    "#         print(np.array(self.W).shape)\n",
    "#         print(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        assert len(label_set) == 2\n",
    "        binary_dict = {label_set[0]:0, label_set[1]:1}\n",
    "        return lambda labels: np.array([binary_dict[label] for label in labels]).reshape(-1, 1)\n",
    "    \n",
    "    def logistic(self, s):\n",
    "        return np.array(1 / (1 + np.exp(-s)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.logistic(np.dot(X, self.w))\n",
    "        return np.array([1 for prob in probs if prob>0.5])\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        print(\"logistic losses: {}\".format(- np.dot(y.T, np.log(y_hat)) / len(y_hat)))\n",
    "#         return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "        return (- np.dot(y.T, np.log(y_hat)) / len(y_hat))[0][0]\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        return np.sum((y_hat - y) * X, axis=0).reshape(-1, 1) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        one_hot_dict = {label:one_hot for label, one_hot in zip(label_set, np.eye(len(label_set)))}\n",
    "        return lambda labels: np.array([one_hot_dict[label] for label in labels])\n",
    "    \n",
    "    def softmax(self, s):\n",
    "        return np.exp(s) / np.sum(np.exp(s), axis=1, keepdims=True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.softmax(np.dot(X, self.w))\n",
    "        return np.argmax(probs, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        print(\"softmax losses: {}\".format(- np.sum(y * np.log(y_hat)) / len(y_hat)))\n",
    "        return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        return np.dot(X.T, (y_hat - y)) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Records:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.holdout_losses = []\n",
    "        self.test_accuracies = []\n",
    "                \n",
    "    def record(self, train_los, holdout_los, test_acc):\n",
    "        self.train_losses.append(train_los)\n",
    "        self.holdout_losses.append(holdout_los)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        \n",
    "    def plt_losses(self, n_components, lr, n_epoches, train=True, holdout=True):\n",
    "        assert len(self.train_losses) == len(self.holdout_losses)\n",
    "        plt.figure()\n",
    "        if train:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.train_losses, axis=0), yerr=np.std(self.train_losses, axis=0))\n",
    "        if holdout:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.holdout_losses, axis=0), yerr=np.std(self.holdout_losses, axis=0))\n",
    "        plt.title(\"n_components={}, learning_rates={}, n_epoches={}\".format(n_components, lr, n_epoches))\n",
    "        plt.show()\n",
    "        \n",
    "    def show_accuracies(self):\n",
    "        print(\"The accuracy is {}\".format(np.mean(self.test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, facial_expressions, dataset_dir='./CAFE/', classifier_type=\"logistic\", pca=True):\n",
    "        self.facial_expressions = facial_expressions\n",
    "        \n",
    "        self.data_builder = DataBuilder()\n",
    "        self.data_builder.load_data(dataset_dir)\n",
    "        \n",
    "        if classifier_type==\"logistic\":\n",
    "            self.classifier = LogisticClassifier()\n",
    "        elif classifier_type==\"softmax\":\n",
    "            self.classifier = SoftmaxClassifier()\n",
    "            \n",
    "        if pca:\n",
    "            self.PCA = PCA()\n",
    "            \n",
    "        self.records = Records()\n",
    "        \n",
    "    def build(self, n_components, learning_rate, n_epoches, batch_size=None, n_repeats=10):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoches = n_epoches\n",
    "        self.batch_size = batch_size\n",
    "        self.n_repeats = n_repeats\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        subjects = self.data_builder.get_subject_ids()\n",
    "        print(\"subject: {}\".format(subjects))\n",
    "        for repeat in range(self.n_repeats):\n",
    "            for test_subject in subjects:\n",
    "                # Dataset building\n",
    "                train, holdout, test, pca_data = self.data_builder.build_dataset(test_subject, self.facial_expressions)\n",
    "                self.PCA.learn(pca_data)\n",
    "                \n",
    "                train.data = self.PCA.run(train.data, self.n_components)\n",
    "                holdout.data = self.PCA.run(holdout.data, self.n_components)\n",
    "                test.data = self.PCA.run(test.data, self.n_components)\n",
    "\n",
    "                # Run classification algorithm\n",
    "                self.classifier.load_data(train, holdout, test)\n",
    "                self.classifier.train(lr=self.learning_rate, T=self.n_epoches, bs=self.batch_size)\n",
    "                self.classifier.test()\n",
    "                self.records.record(self.classifier.train_losses, self.classifier.holdout_losses, self.classifier.test_accuracies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: ['043', '044', '036', '018', '041', '048ng', '037', '049', '027', '050']\n",
      "train losses\n",
      "logistic losses: [[0.2732588]]\n",
      "0.27325879607062525\n",
      "logistic losses: [[0.2732588]]\n",
      "logistic losses: [[0.37695793]]\n",
      "train losses\n",
      "logistic losses: [[0.21614792]]\n",
      "0.21614792355311768\n",
      "logistic losses: [[0.21614792]]\n",
      "logistic losses: [[0.39510337]]\n",
      "train losses\n",
      "logistic losses: [[0.18410493]]\n",
      "0.1841049282145388\n",
      "logistic losses: [[0.18410493]]\n",
      "logistic losses: [[0.42566045]]\n",
      "train losses\n",
      "logistic losses: [[0.14883961]]\n",
      "0.14883961255996636\n",
      "logistic losses: [[0.14883961]]\n",
      "logistic losses: [[0.42891822]]\n",
      "train losses\n",
      "logistic losses: [[0.13619309]]\n",
      "0.1361930912490127\n",
      "logistic losses: [[0.13619309]]\n",
      "logistic losses: [[0.46460987]]\n",
      "train losses\n",
      "logistic losses: [[0.11162216]]\n",
      "0.11162215584296163\n",
      "logistic losses: [[0.11162216]]\n",
      "logistic losses: [[0.45690132]]\n",
      "train losses\n",
      "logistic losses: [[0.10551238]]\n",
      "0.10551237620386257\n",
      "logistic losses: [[0.10551238]]\n",
      "logistic losses: [[0.49073598]]\n",
      "train losses\n",
      "logistic losses: [[0.09005761]]\n",
      "0.09005760855837167\n",
      "logistic losses: [[0.09005761]]\n",
      "logistic losses: [[0.48569181]]\n",
      "train losses\n",
      "logistic losses: [[0.08431866]]\n",
      "0.08431865526782184\n",
      "logistic losses: [[0.08431866]]\n",
      "logistic losses: [[0.50750725]]\n",
      "train losses\n",
      "logistic losses: [[0.07560526]]\n",
      "0.07560526014931215\n",
      "logistic losses: [[0.07560526]]\n",
      "logistic losses: [[0.5112957]]\n",
      "train losses\n",
      "logistic losses: [[0.2773125]]\n",
      "0.2773124992158805\n",
      "logistic losses: [[0.2773125]]\n",
      "logistic losses: [[0.3651931]]\n",
      "train losses\n",
      "logistic losses: [[0.21872863]]\n",
      "0.2187286285056267\n",
      "logistic losses: [[0.21872863]]\n",
      "logistic losses: [[0.36802415]]\n",
      "train losses\n",
      "logistic losses: [[0.19091867]]\n",
      "0.1909186739041507\n",
      "logistic losses: [[0.19091867]]\n",
      "logistic losses: [[0.3926469]]\n",
      "train losses\n",
      "logistic losses: [[0.15070968]]\n",
      "0.1507096831231322\n",
      "logistic losses: [[0.15070968]]\n",
      "logistic losses: [[0.37815343]]\n",
      "train losses\n",
      "logistic losses: [[0.14401739]]\n",
      "0.14401739316446216\n",
      "logistic losses: [[0.14401739]]\n",
      "logistic losses: [[0.41514265]]\n",
      "train losses\n",
      "logistic losses: [[0.11294304]]\n",
      "0.11294304426975202\n",
      "logistic losses: [[0.11294304]]\n",
      "logistic losses: [[0.38676955]]\n",
      "train losses\n",
      "logistic losses: [[0.11235791]]\n",
      "0.11235790661468215\n",
      "logistic losses: [[0.11235791]]\n",
      "logistic losses: [[0.42537175]]\n",
      "train losses\n",
      "logistic losses: [[0.09208899]]\n",
      "0.09208898662460595\n",
      "logistic losses: [[0.09208899]]\n",
      "logistic losses: [[0.40203119]]\n",
      "train losses\n",
      "logistic losses: [[0.08915137]]\n",
      "0.08915136658485526\n",
      "logistic losses: [[0.08915137]]\n",
      "logistic losses: [[0.42534019]]\n",
      "train losses\n",
      "logistic losses: [[0.07849236]]\n",
      "0.07849235645081155\n",
      "logistic losses: [[0.07849236]]\n",
      "logistic losses: [[0.41764761]]\n",
      "train losses\n",
      "logistic losses: [[0.2724607]]\n",
      "0.27246069698702435\n",
      "logistic losses: [[0.2724607]]\n",
      "logistic losses: [[0.36732936]]\n",
      "train losses\n",
      "logistic losses: [[0.21530079]]\n",
      "0.2153007901340634\n",
      "logistic losses: [[0.21530079]]\n",
      "logistic losses: [[0.37735187]]\n",
      "train losses\n",
      "logistic losses: [[0.18279835]]\n",
      "0.18279835328704824\n",
      "logistic losses: [[0.18279835]]\n",
      "logistic losses: [[0.39897478]]\n",
      "train losses\n",
      "logistic losses: [[0.14799245]]\n",
      "0.14799244607055667\n",
      "logistic losses: [[0.14799245]]\n",
      "logistic losses: [[0.39671872]]\n",
      "train losses\n",
      "logistic losses: [[0.13477948]]\n",
      "0.13477948276841228\n",
      "logistic losses: [[0.13477948]]\n",
      "logistic losses: [[0.42330492]]\n",
      "train losses\n",
      "logistic losses: [[0.11089618]]\n",
      "0.11089618423198427\n",
      "logistic losses: [[0.11089618]]\n",
      "logistic losses: [[0.41257489]]\n",
      "train losses\n",
      "logistic losses: [[0.10423902]]\n",
      "0.10423902159051941\n",
      "logistic losses: [[0.10423902]]\n",
      "logistic losses: [[0.4376136]]\n",
      "train losses\n",
      "logistic losses: [[0.08933525]]\n",
      "0.0893352516926824\n",
      "logistic losses: [[0.08933525]]\n",
      "logistic losses: [[0.43017292]]\n",
      "train losses\n",
      "logistic losses: [[0.0833185]]\n",
      "0.08331849996137455\n",
      "logistic losses: [[0.0833185]]\n",
      "logistic losses: [[0.44511724]]\n",
      "train losses\n",
      "logistic losses: [[0.0748514]]\n",
      "0.07485139557258724\n",
      "logistic losses: [[0.0748514]]\n",
      "logistic losses: [[0.4456234]]\n",
      "train losses\n",
      "logistic losses: [[0.2790937]]\n",
      "0.27909369793799144\n",
      "logistic losses: [[0.2790937]]\n",
      "logistic losses: [[0.35696698]]\n",
      "train losses\n",
      "logistic losses: [[0.21927642]]\n",
      "0.21927642244790627\n",
      "logistic losses: [[0.21927642]]\n",
      "logistic losses: [[0.34853684]]\n",
      "train losses\n",
      "logistic losses: [[0.19437642]]\n",
      "0.1943764199149547\n",
      "logistic losses: [[0.19437642]]\n",
      "logistic losses: [[0.36737397]]\n",
      "train losses\n",
      "logistic losses: [[0.15096081]]\n",
      "0.15096081211683338\n",
      "logistic losses: [[0.15096081]]\n",
      "logistic losses: [[0.33997427]]\n",
      "train losses\n",
      "logistic losses: [[0.14835189]]\n",
      "0.1483518873977245\n",
      "logistic losses: [[0.14835189]]\n",
      "logistic losses: [[0.37458617]]\n",
      "train losses\n",
      "logistic losses: [[0.11308932]]\n",
      "0.11308932126399193\n",
      "logistic losses: [[0.11308932]]\n",
      "logistic losses: [[0.3327139]]\n",
      "train losses\n",
      "logistic losses: [[0.11622837]]\n",
      "0.11622837054939208\n",
      "logistic losses: [[0.11622837]]\n",
      "logistic losses: [[0.37024349]]\n",
      "train losses\n",
      "logistic losses: [[0.09283551]]\n",
      "0.09283551072284257\n",
      "logistic losses: [[0.09283551]]\n",
      "logistic losses: [[0.33549194]]\n",
      "train losses\n",
      "logistic losses: [[0.09184141]]\n",
      "0.09184141098431928\n",
      "logistic losses: [[0.09184141]]\n",
      "logistic losses: [[0.35638809]]\n",
      "train losses\n",
      "logistic losses: [[0.0798988]]\n",
      "0.0798987969010006\n",
      "logistic losses: [[0.0798988]]\n",
      "logistic losses: [[0.34082965]]\n",
      "train losses\n",
      "logistic losses: [[0.27829379]]\n",
      "0.2782937853415064\n",
      "logistic losses: [[0.27829379]]\n",
      "logistic losses: [[0.27497712]]\n",
      "train losses\n",
      "logistic losses: [[0.21749456]]\n",
      "0.21749455990802702\n",
      "logistic losses: [[0.21749456]]\n",
      "logistic losses: [[0.21182069]]\n",
      "train losses\n",
      "logistic losses: [[0.19256456]]\n",
      "0.19256456380165343\n",
      "logistic losses: [[0.19256456]]\n",
      "logistic losses: [[0.18401272]]\n",
      "train losses\n",
      "logistic losses: [[0.14859716]]\n",
      "0.14859716135095535\n",
      "logistic losses: [[0.14859716]]\n",
      "logistic losses: [[0.13964244]]\n",
      "train losses\n",
      "logistic losses: [[0.14614887]]\n",
      "0.14614886892861062\n",
      "logistic losses: [[0.14614887]]\n",
      "logistic losses: [[0.13455466]]\n",
      "train losses\n",
      "logistic losses: [[0.11086925]]\n",
      "0.11086925477220477\n",
      "logistic losses: [[0.11086925]]\n",
      "logistic losses: [[0.10045278]]\n",
      "train losses\n",
      "logistic losses: [[0.11378279]]\n",
      "0.11378278625869012\n",
      "logistic losses: [[0.11378279]]\n",
      "logistic losses: [[0.10115093]]\n",
      "train losses\n",
      "logistic losses: [[0.09089578]]\n",
      "0.09089577522558709\n",
      "logistic losses: [[0.09089578]]\n",
      "logistic losses: [[0.07959333]]\n",
      "train losses\n",
      "logistic losses: [[0.08950033]]\n",
      "0.08950032758859994\n",
      "logistic losses: [[0.08950033]]\n",
      "logistic losses: [[0.07709807]]\n",
      "train losses\n",
      "logistic losses: [[0.07803374]]\n",
      "0.07803373802461593\n",
      "logistic losses: [[0.07803374]]\n",
      "logistic losses: [[0.0662734]]\n",
      "train losses\n",
      "logistic losses: [[0.27345408]]\n",
      "0.2734540802974186\n",
      "logistic losses: [[0.27345408]]\n",
      "logistic losses: [[0.29886386]]\n",
      "train losses\n",
      "logistic losses: [[0.21530333]]\n",
      "0.21530332716095496\n",
      "logistic losses: [[0.21530333]]\n",
      "logistic losses: [[0.25654115]]\n",
      "train losses\n",
      "logistic losses: [[0.18449246]]\n",
      "0.18449245986840812\n",
      "logistic losses: [[0.18449246]]\n",
      "logistic losses: [[0.23819298]]\n",
      "train losses\n",
      "logistic losses: [[0.14764883]]\n",
      "0.14764882565505155\n",
      "logistic losses: [[0.14764883]]\n",
      "logistic losses: [[0.20603832]]\n",
      "train losses\n",
      "logistic losses: [[0.13677439]]\n",
      "0.13677438782080997\n",
      "logistic losses: [[0.13677439]]\n",
      "logistic losses: [[0.20412024]]\n",
      "train losses\n",
      "logistic losses: [[0.11056219]]\n",
      "0.11056218749674238\n",
      "logistic losses: [[0.11056219]]\n",
      "logistic losses: [[0.17678251]]\n",
      "train losses\n",
      "logistic losses: [[0.10580388]]\n",
      "0.10580388472416631\n",
      "logistic losses: [[0.10580388]]\n",
      "logistic losses: [[0.17901256]]\n",
      "train losses\n",
      "logistic losses: [[0.08945014]]\n",
      "0.08945013717013064\n",
      "logistic losses: [[0.08945014]]\n",
      "logistic losses: [[0.16061442]]\n",
      "train losses\n",
      "logistic losses: [[0.08423943]]\n",
      "0.08423942659569633\n",
      "logistic losses: [[0.08423943]]\n",
      "logistic losses: [[0.15904174]]\n",
      "train losses\n",
      "logistic losses: [[0.07530503]]\n",
      "0.07530502666947611\n",
      "logistic losses: [[0.07530503]]\n",
      "logistic losses: [[0.14949936]]\n",
      "train losses\n",
      "logistic losses: [[0.27216662]]\n",
      "0.27216662283732473\n",
      "logistic losses: [[0.27216662]]\n",
      "logistic losses: [[0.32024425]]\n",
      "train losses\n",
      "logistic losses: [[0.21734794]]\n",
      "0.21734794131452562\n",
      "logistic losses: [[0.21734794]]\n",
      "logistic losses: [[0.29633069]]\n",
      "train losses\n",
      "logistic losses: [[0.18267874]]\n",
      "0.182678735208284\n",
      "logistic losses: [[0.18267874]]\n",
      "logistic losses: [[0.28339029]]\n",
      "train losses\n",
      "logistic losses: [[0.15126824]]\n",
      "0.15126824168500905\n",
      "logistic losses: [[0.15126824]]\n",
      "logistic losses: [[0.26420463]]\n",
      "train losses\n",
      "logistic losses: [[0.13464789]]\n",
      "0.13464789300410684\n",
      "logistic losses: [[0.13464789]]\n",
      "logistic losses: [[0.26001692]]\n",
      "train losses\n",
      "logistic losses: [[0.11420737]]\n",
      "0.11420736649140767\n",
      "logistic losses: [[0.11420737]]\n",
      "logistic losses: [[0.24316186]]\n",
      "train losses\n",
      "logistic losses: [[0.10484491]]\n",
      "0.10484490541067629\n",
      "logistic losses: [[0.10484491]]\n",
      "logistic losses: [[0.24201729]]\n",
      "train losses\n",
      "logistic losses: [[0.09182353]]\n",
      "0.09182353393315669\n",
      "logistic losses: [[0.09182353]]\n",
      "logistic losses: [[0.22979587]]\n",
      "train losses\n",
      "logistic losses: [[0.08472783]]\n",
      "0.08472782789823412\n",
      "logistic losses: [[0.08472783]]\n",
      "logistic losses: [[0.22717574]]\n",
      "train losses\n",
      "logistic losses: [[0.07671783]]\n",
      "0.07671782888173713\n",
      "logistic losses: [[0.07671783]]\n",
      "logistic losses: [[0.21996902]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train losses\n",
      "logistic losses: [[0.27254622]]\n",
      "0.2725462192790694\n",
      "logistic losses: [[0.27254622]]\n",
      "logistic losses: [[0.3087149]]\n",
      "train losses\n",
      "logistic losses: [[0.21610481]]\n",
      "0.21610481186815467\n",
      "logistic losses: [[0.21610481]]\n",
      "logistic losses: [[0.27357772]]\n",
      "train losses\n",
      "logistic losses: [[0.18302681]]\n",
      "0.1830268078438081\n",
      "logistic losses: [[0.18302681]]\n",
      "logistic losses: [[0.25402765]]\n",
      "train losses\n",
      "logistic losses: [[0.14912787]]\n",
      "0.14912787453413084\n",
      "logistic losses: [[0.14912787]]\n",
      "logistic losses: [[0.22579182]]\n",
      "train losses\n",
      "logistic losses: [[0.13501607]]\n",
      "0.1350160694486887\n",
      "logistic losses: [[0.13501607]]\n",
      "logistic losses: [[0.21825581]]\n",
      "train losses\n",
      "logistic losses: [[0.11197511]]\n",
      "0.11197511305166852\n",
      "logistic losses: [[0.11197511]]\n",
      "logistic losses: [[0.19426647]]\n",
      "train losses\n",
      "logistic losses: [[0.10466715]]\n",
      "0.10466715419815764\n",
      "logistic losses: [[0.10466715]]\n",
      "logistic losses: [[0.19079453]]\n",
      "train losses\n",
      "logistic losses: [[0.09015299]]\n",
      "0.09015298859100918\n",
      "logistic losses: [[0.09015299]]\n",
      "logistic losses: [[0.17384447]]\n",
      "train losses\n",
      "logistic losses: [[0.08391386]]\n",
      "0.08391386124591041\n",
      "logistic losses: [[0.08391386]]\n",
      "logistic losses: [[0.16879019]]\n",
      "train losses\n",
      "logistic losses: [[0.07549993]]\n",
      "0.07549992601341265\n",
      "logistic losses: [[0.07549993]]\n",
      "logistic losses: [[0.15867068]]\n",
      "train losses\n",
      "logistic losses: [[0.27629221]]\n",
      "0.276292212527314\n",
      "logistic losses: [[0.27629221]]\n",
      "logistic losses: [[0.32383261]]\n",
      "train losses\n",
      "logistic losses: [[0.21626961]]\n",
      "0.21626960591697753\n",
      "logistic losses: [[0.21626961]]\n",
      "logistic losses: [[0.29366146]]\n",
      "train losses\n",
      "logistic losses: [[0.18925958]]\n",
      "0.18925958384198094\n",
      "logistic losses: [[0.18925958]]\n",
      "logistic losses: [[0.2911575]]\n",
      "train losses\n",
      "logistic losses: [[0.14786344]]\n",
      "0.1478634429167168\n",
      "logistic losses: [[0.14786344]]\n",
      "logistic losses: [[0.25785309]]\n",
      "train losses\n",
      "logistic losses: [[0.14233685]]\n",
      "0.1423368532376491\n",
      "logistic losses: [[0.14233685]]\n",
      "logistic losses: [[0.27218861]]\n",
      "train losses\n",
      "logistic losses: [[0.11054638]]\n",
      "0.11054638436603531\n",
      "logistic losses: [[0.11054638]]\n",
      "logistic losses: [[0.23602785]]\n",
      "train losses\n",
      "logistic losses: [[0.11045095]]\n",
      "0.11045095191820889\n",
      "logistic losses: [[0.11045095]]\n",
      "logistic losses: [[0.25329264]]\n",
      "train losses\n",
      "logistic losses: [[0.09027157]]\n",
      "0.0902715671204441\n",
      "logistic losses: [[0.09027157]]\n",
      "logistic losses: [[0.22697567]]\n",
      "train losses\n",
      "logistic losses: [[0.0872579]]\n",
      "0.08725790285407416\n",
      "logistic losses: [[0.0872579]]\n",
      "logistic losses: [[0.23399035]]\n",
      "train losses\n",
      "logistic losses: [[0.0769241]]\n",
      "0.07692409921501724\n",
      "logistic losses: [[0.0769241]]\n",
      "logistic losses: [[0.22183304]]\n",
      "train losses\n",
      "logistic losses: [[0.27254622]]\n",
      "0.2725462192790694\n",
      "logistic losses: [[0.27254622]]\n",
      "logistic losses: [[0.32177078]]\n",
      "train losses\n",
      "logistic losses: [[0.21610481]]\n",
      "0.21610481186815467\n",
      "logistic losses: [[0.21610481]]\n",
      "logistic losses: [[0.29611162]]\n",
      "train losses\n",
      "logistic losses: [[0.18302681]]\n",
      "0.1830268078438081\n",
      "logistic losses: [[0.18302681]]\n",
      "logistic losses: [[0.28474559]]\n",
      "train losses\n",
      "logistic losses: [[0.14912787]]\n",
      "0.14912787453413084\n",
      "logistic losses: [[0.14912787]]\n",
      "logistic losses: [[0.26121597]]\n",
      "train losses\n",
      "logistic losses: [[0.13501607]]\n",
      "0.1350160694486887\n",
      "logistic losses: [[0.13501607]]\n",
      "logistic losses: [[0.25994001]]\n",
      "train losses\n",
      "logistic losses: [[0.11197511]]\n",
      "0.11197511305166852\n",
      "logistic losses: [[0.11197511]]\n",
      "logistic losses: [[0.23767661]]\n",
      "train losses\n",
      "logistic losses: [[0.10466715]]\n",
      "0.10466715419815764\n",
      "logistic losses: [[0.10466715]]\n",
      "logistic losses: [[0.23915404]]\n",
      "train losses\n",
      "logistic losses: [[0.09015299]]\n",
      "0.09015298859100918\n",
      "logistic losses: [[0.09015299]]\n",
      "logistic losses: [[0.22301412]]\n",
      "train losses\n",
      "logistic losses: [[0.08391386]]\n",
      "0.08391386124591041\n",
      "logistic losses: [[0.08391386]]\n",
      "logistic losses: [[0.22108756]]\n",
      "train losses\n",
      "logistic losses: [[0.07549993]]\n",
      "0.07549992601341265\n",
      "logistic losses: [[0.07549993]]\n",
      "logistic losses: [[0.21201664]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4FeX1wPHvyUZWIEASdsISdgQkgBuCAoobWFstWBS3IlZ/2trWtra11qVVq7a2tSoqbqi4VCtVLG5sLiwBEWTfZSeyBxKSkPP7452Qm5iQG0gyyb3n8zzz3DvbvWfmzpx35n3nzoiqYowxJjxE+B2AMcaY2mNJ3xhjwoglfWOMCSOW9I0xJoxY0jfGmDBiSd8YY8KIJX3jOxG5W0Qm+/TdT4rI7/34blO3iMhMEbnB7zhqmiX9ECQiQ0RkSw1+/tUiooE7iDgPishur3tQRKSmYqguqjpBVe/1O45AfhSCInKviCwVkUIRubvMuBYiMlVEtnm/e3ptxhYOKlvHItJARCaJyAER2SEit5/od1nSN1UiIsnAncCyMqPGA5cCvYFTgEuAG2s3utJEJMrP7y9PXYzJsxa4A3ivnHFFwP+A79dqROGlsnV8N5ABtAPOAe4QkREn9E2qWuc7YCPwC2AJsB94DYgNYr5RwGLgALAOGOENbwlMBfbgNvYfB8xzN/AGMBk4CCwFOgO/AXYBm4HzAqafCfwZmO99zztAk4DxI3EJcp83bbdglwu42It/H/A5cEpl8wIJQC5uI8rxupbAACDLi3En8OgJ/hZPAj/xluWGgOGfA+MD+q8H5gb5mXcDkwP6T/M+bx/wFTAkYNy1wArvt1kP3BgwbgiwBfgVsAN4KWDYz73fbztwbcA8zwP3lZm/ommbAv/11uEC4D7g0yCWT4GbgTXABm/YY962dABYCAzyho8A8oEC77f7yhveCHjWi2mr992R3rhOwCxvO/gWeO0k9rXJwN0VjIvyliW9Cp+X7s0zDvjGi++3QcwXAfwat9/uBl7H268CPnM8sM1bJ78ImLcB8Ddv3DbvfYMg8sJM4F7gM2/7+gBoFuR2eY23PR4ENgA/OsH1X+469pYjMO/cC0w5oe840Y2jNjtcgpuPS15NcDv9hErmGeDtBMO9DagV0NUbNxv4Fy5J9gGygXO9cXcDecD53g/wovcj/haIBn6Mt+MGbChbgZ64hPtvvASGKywOeTFE446k1gIxlS0X0BeXeAYCkd5Os7F4461k3iHAljLr4wvgKu99InBawLh9x+l+XWadZnnrcyalk/5+YGBAfyZwMMjf9+6AddYKt5Nf6H3PcK8/xRt/EdAREGAwcBg4NWC5C4EHcTt+XMCwe7zf4EJvnmRvnucpnfSPN+0Ur4sHuuOSdrBJ/0Pvd4rzho3FFSJRuEJmB16BT5lC0Bv2NvAUbhtL9X77G71xr+K2zwjcNn1WwHxLjvPb/qucWGsq6T/t/R69gSMEHPxUMN9twFygtfdbPgW8WuYzX/XWRy/cPjzMG3+PN28qkIJL1PcGkRdm4gqBzl6sM4EHKtsuvRgOAF28aVsAPbz3Zx1n/e8L/K0qWsdAsjcsLWDYD4ClJ5RPTyYZ11aHS3BjA/ofAp6sZJ6ngL+WM7wNcBRIChj2Z+D5gB3uw4Bxl+COuIqPqpK8H6BxwIbyQMD03XFHapHA74HXA8ZF4AqIIZUtF/BE8YYaMH4VMDiIeYfw3aQ/G/gjAUcuVfwNInEJ/7SA5Q5M+keLdx6vP8NbTxLEZ99NSdL/FfBSmfHTgXEVzPsf4LaA5c6n9NnSENyZT1TAsF0By/E8pZN+udN6y1+At2N746pypH9uJdPsBXqXXR9efxouUcYFDBsDzPDevwhMBFpXw75WU0m/dcCw+cDoSuZbAQwN6G/hrf+ogM8M3N4eAp713q8DLgwYdz6w0Xtfbl4I2KZ/F9D/E+B/lW2XuKS/D1c1E3e85QpifZWX9Nt4wwK36+HFy1TVrj7V6e8IeH8Yd7R6PG1wP35ZLYE9qnowYNgmXElebGfA+1zgW1U9GtBPme/fXOazooFm3ndtKh6hqkXetIHfVdFytQN+LiL7ijtvmVoGMW95rscdwawUkQUicvFxpi3PT4Alqjq3gvE5QMOA/oZAjnpbaBW0Ay4vs9xn4XZ6ROQCEZkrInu8cRfi1nWxbFXNK/OZu1W1MKD/eOuqomlTcDtk4G8d+L4ypaYVkV+IyAoR2e8tR6MyyxGoHW6b2h6wTp7CHcmCO4MUYL6ILBOR66oQV22p6v7bDng7YHlX4A4s0gKmKbvfFe8bpfa7MuMqyguVxVnhdqmqh4AfAhNwv9F7ItK1kuWrihzvtez+dbCcaStVn5J+VW3GVQOUtQ1oIiJJAcPa4o7AT1SbMp9VgKu73IbbWAB3hYs3bTDftRm4X1UbB3TxqvpqEPN+J9Gq6hpVHYNLFA8Cb4pIghdXznG6O72PGAp8z7tyYAdwBvCIiPzTG78Md+perDffbewNxmbcEVXgcieo6gMi0gBXffYw7lS3MTANl/AqXPZqko2r+mkdMKxNBdOW51hcIjIIl6ivwFUdNcZVOUjZaT2bcUf6zQLWSUNV7QGgqjtU9ceq2hLXeP4vEenkfdey4/y2T1Yh/tq2GbigzHYQq6qB+07Z/W6b977UfldmXEV5IZh4yt0uAVR1uqoOxx2crMRVZyEigyrZvwZV9sWquhfXblEd+1dIJ/1ngWtFZKiIRIhIKxHpqqqbcXV8fxaRWBE5BXcUfDKXyI0Vke4iEo+rT3zTOzN4HbjIiyEaV3d7xPv+yjwNTBCRgd7lkAkiclGZwqoiO4GmItKoeICIjBWRFO9sY583uAhAVROP0/3Jm/YaoBuuDaQPrqrnj7i6ZHBVDLd767mlt6zPB3z/RhG5JojYJwOXiMj5IhLp/UZDRKQ1EIOr380GCkXkAuC8ID7zpHm/51vA3SIS7x3JXX2CH5eEK0CygSgRuYvSR3E7gXQRifC+ezuuUfEREWnobc8dRWQwgIhc7q0fcNVESslv2+M4v+2E4i8UkWgRicXlhChvvUcGjI/FrXuABl5/8bi7RWTmCa6LijwJ3C8i7bzvSBGRUWWm+b33W/TANfC/5g1/FfidN08z4C5K9u9y80IQ8VS4XYpImoiM8g6ijuCOzIvX/5xK9q85xV9wvHWM279+JyLJXrw/JmD/qoqQTfqqOh+3IfwVdxQ1i5LSfwyuXnAbroHsD6r60Ul83Uu4H2AHriHtVi+GVbgGu3/gjvwvAS5R1fwg4s/C/bD/xO3Ia3GJt1KquhK34a/3TkVb4q4KWSYiObgrR0arau7xPqfMZ+7zjih3qOoOXN35AVXd703yFO7KlqXA17hL/54CEJEYXKNlRVVDgd+zGXd1xZ24pLgZ+CUQ4VXJ3YorTPcCV+Kuwqott+CqYYqvDHoVt5NX1XTc5XmrcVUPeZSuqnjDe90tIou891fjCr3luGV/E6/KC+gPzPN+26m4No71VYzpaVzV5RhcQZ4LXBUwPpeSaoaVlFRzgjvi/qyK31eZx3DL8oGIHMRtOwPLTDMLt198DDysqh94w+/DHZQswW2Pi7xhleWFCh1vu/S623H5ZA/uAoObTmCZj7eO/4CrltrkxfwXVf3fCXyHa2QzJ847wpmsqs/4HUtdJSJnATd71UshQ0QeBJqr6ji/Y/GTiCzGNbrurqXvS8ddURddpv3FBKGu/lHEhBBV/RT41O84TpZ3Wh2DO3rsj6sWDPm/7VdGVfv4HYMJXr2u3hGROytoHHnf79hMSErC1esfwtUfPwK8c7zGOl+jreNE5P0K1tudlc9tTpRV7xhjTBip10f6xhhjqqbO1ek3a9ZM09PT/Q7DGGPqlYULF36rqimVTVfnkn56ejpZWVl+h2GMMfWKiGyqfCqr3jHGmLBiSd8YY8KIJX1jjAkjlvSNMSaMWNI3xpgwYknfGGPCiCV9Y4wJI5b0jTEmjFjSN8aEt+cucl2YsKRvTDgKs0RnSljSN8aYMGJJ3xhj6oJaOvuypG+MMWHEkr4xxoQRS/omPFjDpTFAkElfREaIyCoRWSsivz7OdN8XERWRzIBhv/HmWyUi51dH0MYYY05MpQ9REZFI4HFgOLAFWCAiU1V1eZnpkoDbgHkBw7oDo4EeQEvgIxHprKpHq28RjDHGBCuYI/0BwFpVXa+q+cAUYFQ5090LPAjkBQwbBUxR1SOqugFY632eMcYYHwST9FsBmwP6t3jDjhGRU4E2qvpeVef15h8vIlkikpWdnR1U4MaYes7aWXxx0g25IhIBPAr8/EQ/Q1UnqmqmqmampFT6XF9jjDEnKJgHo28F2gT0t/aGFUsCegIzRQSgOTBVREYGMa8xxphaFMyR/gIgQ0Tai0gMrmF2avFIVd2vqs1UNV1V04G5wEhVzfKmGy0iDUSkPZABzK/2pTDGGBOUSo/0VbVQRG4BpgORwCRVXSYi9wBZqjr1OPMuE5HXgeVAIXCzXbljjDH+CaZ6B1WdBkwrM+yuCqYdUqb/fuD+E4zPGGNMNbJ/5BpjTBixpB+q7HI4Y0w5LOkbU5usMDY+s6RvjDFhxJK+McaEEUv6xhgTRizpG2NMGLGkb2qWNVwaU6dY0jfGmDBiSd8YY8KIJX1jjAkjlvSNMSaMWNI3xpgwYkm/utnVKsaYOsySvjHGhJHQSvp2lG2MMccVWknfGGPMcVnSN8aYMGJJ3xhjwoglfWOMCSNBJX0RGSEiq0RkrYj8upzxE0RkqYgsFpFPRaS7NzxdRHK94YtF5MnqXgBjjDHBi6psAhGJBB4HhgNbgAUiMlVVlwdM9oqqPulNPxJ4FBjhjVunqn2qN2xjjDEnIpgj/QHAWlVdr6r5wBRgVOAEqnogoDcB0OoL0RhjTHUJJum3AjYH9G/xhpUiIjeLyDrgIeDWgFHtReRLEZklIoPK+wIRGS8iWSKSlZ2dXYXwjTHGVEW1NeSq6uOq2hH4FfA7b/B2oK2q9gVuB14RkYblzDtRVTNVNTMlJaW6QjLGGFNGMEl/K9AmoL+1N6wiU4BLAVT1iKru9t4vBNYBnU8sVGOMMScrmKS/AMgQkfYiEgOMBqYGTiAiGQG9FwFrvOEpXkMwItIByADWV0fgxhhjqq7Sq3dUtVBEbgGmA5HAJFVdJiL3AFmqOhW4RUSGAQXAXmCcN/vZwD0iUgAUARNUdU9NLIgxxpjKVZr0AVR1GjCtzLC7At7fVsF8/wb+fTIBGmOMqT72j1xjjAkjlvSNMSaMWNI3xpgwElpJv+io3xEYY0ydFjpJP28/bJkHO7+GRS/CYbtIyBhjygqdpH+0EJJaQmEeTP0/eDgDXr4cFr8Cufv8js4YY+qEoC7ZrBcSmkJyOjRuB+ffB8vehmX/gTU3QWQMdBwKPS+DziMg9jt3gjDGmLAQOkm/mAi0OtV1w++BrQvh67dg+X9g9fsQ2QAyhkOP77kCoEGi3xEbY0ytCb2kH0gEWme67rz7YMv8kjOAle9CVBx0Pg96XAYZ50FMvN8RG2NMjQrtpB8oIgLanua68/8E38yFZW/B8ndcF50AXUa4M4BOwyE61u+IjTGm2oVP0g8UEQnpZ7rugodg46fuDGDFVPj63xCTBF0ucG0AHc+FqAZ+R2yMMdUiPJN+oIhI6DDYdRc+DBtnuzaAFf+Fpa9Dg0bQ9SJ3BtBhCETF+B2xMcacMEv6gSKj3JF9x3Ph4r/C+pneGcC78NUrENsYul3s2gDanw2R0X5HbIwxVWJJvyKR0e4qn4zhrgBYN8O1ASx7B76cDHFNoPtIdwbQ7ixXYBhjTB1nmSoYUQ1cI2+XEVCQB2s/cmcAS96Ahc9DQgp0G+naAFTdVUPGGFMHWdKvquhYV8XT7WLIPwxrP3RtAItfgaxn3R/B4prAB78H1BUCqt77Iq+/KGBcRe/LzFPRey1ycZWdf+cy116x4r+Qcb61RRhjAEv6JycmHrqPcl3+IVj9P5j2SziUDfOf9o74BSQi4L18d7hEBIyLqHi6St9TMn9RAeTnwGtjIb4ZnHIF9PkRNO/p08oyxtQFlvSrS0wC9Pw+LJjk+q99z994nrvIHfGfdZtrg5j/NMz9F7ToDX3GQq8fQHwTf2M0xtQ6S/qhTAQ6n++6Q7th6RuweDK8/0v44LfQ5ULoO9ZdrRQR6Xe0xphaYEk/XCQ0hdMmuG77Elj8Mix53d2TKKkF9B7tzgCadfI7UmNMDQrq1soiMkJEVonIWhH5dTnjJ4jIUhFZLCKfikj3gHG/8eZbJSLnV2fw5gS1OAUueBB+vhKueBGanwKfPQb/7AfPngcLX4C8A35HWX3yD7vlKcj1OxJjfFfpkb6IRAKPA8OBLcACEZmqqssDJntFVZ/0ph8JPAqM8JL/aKAH0BL4SEQ6q6o94qouiGpQ0hB9cAd8NcWdAfz3Vnj/V2543x+5/yFE1JNHL6jCnvWwZUFJt+NrKN7kXroMBt7o7q9UX5bJmGoUTPXOAGCtqq4HEJEpwCjgWNJX1cDDwgRAvfejgCmqegTYICJrvc/7ohpiN9UpqTmc9VM48zbYkuXq/r9+C5ZMcc8o6HMl9B4Dye38jrS0vP2wdZGLuTjJ53pPTYtJdLfYPuunsHKau8Jq5zJ45QpIbg8DfuyuaIpr7O8yGFOLgkn6rYDNAf1bgIFlJxKRm4HbgRjg3IB555aZt1U5844HxgO0bds2mLjL5/cVM6FABNr0d935f4aV77kCYOYDMPPP7vYTfcZCt0tq/1bURUWQvTLgKD7L9RcfY6R0ha4XQuv+rkvpWtJA/c08iG8KV3k31ps/EabfCZ/c79ozBoyH1K61uzzG+KDaGnJV9XHgcRG5EvgdMK4K804EJgJkZmZqJZOb2hITD6dc7rp935RU/7w9HqY1dLeg6DvWJdia+Bfyod2wNeAIfusiOOKdVMY2dt/b43vueQmt+gV3xB4V4y5X7fUD2LbYJf8vJ7s/1rUf7Kp+Oo+wq5lMyAom6W8F2gT0t/aGVWQK8MQJzmvqqsZtYfAdMOgX8M3n8OXL7hLQRS9A04yS6p+GLU7s848WuIfaB1bT7FnvxkkkpPWAXpeXHMU37XjyBU3LPnDpv9wT1ha9AAuehSlXumXtfwP0vcr+y2BCTjBJfwGQISLtcQl7NHBl4AQikqGqa7zei4Di91OBV0TkUVxDbgYwvzoCNz6JiID0s1x34UPuKWSLX4aP/wif3OueRdz3R+4/AMd7DsGB7aWrabZ9CYXe1TWJaS6xnzrOvbbs4/78VlMSmsGgn8MZt7knqs2fCB/eBTP+7M5yBtwYGv9kLiqC7BWwfhZ8u8q1eezZAE3a+x2ZqUWVJn1VLRSRW4DpQCQwSVWXicg9QJaqTgVuEZFhQAGwF69qx5vudVyjbyFws125E0IaJMGpV7lu9zqX/Be/Cm9cA3HJ7sj8SI6rJvpmXukkf2CL+4zIGPcv4cxrvUdb9odGbfy5aV1kFPS41HU7lrrkv+QNWPQitDvT1ft3vbj+3FFVFfZucEl+w2zXHf7WjYuIdrcL+XsfSOvpCumuF7nfwm4YGNKC2npVdRowrcywuwLe33acee8H7j/RAE090bQjDL0LzvktrJ/hqn8WvgBHj7jxk85zr43bQtuB0PoWl+Cb96qbTyZr3gtG/gOG/RG+fAnmPwNvjIOGrSDzOuh3jTtDqGsObC9J8BtmwX7vGoykFtBpmGuIb382vD3B/W+h52WwahrMeRhmPwQNW7vG8K4XuYLOnhkRcurJIYupNyIiXXLpNAxy98LTQ6HwiKsKapUJSWl+R1g18U3cZayn3+JuqDfvKVeNNesh1xg8YLyrfvLL4T3ucZ/FSf7b1W54XDKkD3KxdxgCTTt99wg+Og7OuMV1h76F1dPd1VqLXnRnObGN3B1au14EnYa6MztT71nSNzUnLtkdYYJLHPVZRKRbhq4Xwa6VLikWX83UZqBL/t1H1fyRcf4h2PQFbJjpEv32JYBCdAK0O8M1PncYDGm9qvbns4Rmri2m74/cd6yb4c4AVr3vHhsa2cB9bteLoPMF9a/wNsdY0jemqlK7wsWPuuqsxa+4AuDf18P037qqn8xrITG1er6r8IhrAyk+kt+S5W6bHRkDrQfAkN+4ZNzy1Op7ZkJMQskzI44Wwua57s9tK9+FNR8AP3VVc10vcm0cdr+mesWSvjEnKq4xnP4TGDjBPUxn3lMw808w+y/u/wMDb3SN01VRdBS2f+US/IbZ7qi+MNc9J6FFHzj9Zpfk25xWO3+Oi4wquVrr/Pth13JXBbTyXfjoD65r1rmkAGh5av24vcXRQjiw1f3/JGenW++rp0Nyumt3io7zO8IaY0nfmJMVEVFyC+tv17hnFyx+xVWLtDzVJf8e3yu/wVoVsleVJPmNc9ytJcD9o/jUq13Da/qZrrrMTyLu/xJpPdx/NvZtdtU/K9+Fz/4On/4VEptDlwtcAdB+kH+N9EVFcHC7S+r7NrnXvZu895tg/9aS+zEVe+WKkvdJLVwB8J2uvTuLq8dXOFnSN6Y6NctwjdZDf+8uX50/Ed6+ET74HfS71lXXoK6xtPgqm5ydbt7Gbd2zltsPdom+rtebN24DA8e7LncvrPnQFQBLXoeFz0FMEmQMcwVAxnDXMFxdVCFnV0BS3+Qlda9//xY4ml96nsQ0dx+p1gOgVzu3vhu3g4/vdW025/8J9m50l7nu3ei6DbNd2w0BNwqIiqugQEh3n1nbtyepIkv6xtSEBkkuGfa/wV3COn+iq/YpTh5bsyAhteQSyg6DXdKor+KS3SM5T7kCCvJcslz5rjsTWPa2+19A+lmuGqjLhdDoO7fgKk3VXZlUnNCPHakHHLkX5pWeJ76ZS7otert7QzVu53VtXQFVUZXN7Ifda/E9p8oqyHOXvhYXBIHdxjnusaSBEptXXCgkpvle/RVSSf/Cx2aT2CCK1yec4XcoxjgREe5yx05D3W0lXhjpEuDolyG1W72uJqhQdCx0Ps91RUWugFv5rmsLmPYL17Xs65J6g4aw4r/lJ/WyyTS2sUvgKV0g47yShJ7czv2hr0FizS1PswzXlaUKh3cHFATFZwmbYNNnsOQ1Sp8lxLq4k9PdP6EDC4Sio7Vyz6eQSfprd+WwfPtBmibEUHi0iKjIetCYZMJLkw5uhwdI6378aUNFRAS0GeC64fdA9uqSAmDfJjfNa2Pda0ySS+DJ6a6Kq3HbkqTeuG31Vg9VFxF3uWtCs/Ib7QuPuKqmwCqj4m7T55B/sPT0DRrWeMghk/Q7pSbSJjmOzXtz+dnrX/HXK3pb4jemrknpDCm3w6Db4elh7mj+e0+4wjAuOfTOfKIauH+rN+343XGqri2kuED46I/u5oI1HVKNf0MtmvOrc5k4ex1/mraSoiLlb6P7EG2J35i6KaqB61r29TsSf4i4f3zHN3G3Bl8wqVa+NqSSPsD4szsSIcJ9762gSJW/j+lrid8YYzwhmQ1vGNSB31/cnfe/3sEtrywiv7DI75CMMaZOCMmkD3D9We25+5LuTF+2k5st8RtjDBDCSR/gmjPbc8+oHny4fCc/eXkhRwrtVv7GmPAW0kkf4OrT07nv0p58tGIXN01eRF6BJX5jTPgK+aQPMPa0dvzpe734ZOUubnxpoSV+Y0zYCoukD3DlwLY8cFkvZq/J5scvZlniN8aEpbBJ+gCjB7TlwctO4dO13/LjF7PIzbfEb4wJLyF3nX5lrujfhogI4ZdvfsX1Lyzg2XH9iYupxn/BXfte9X2WMcZUs6CO9EVkhIisEpG1IvLrcsbfLiLLRWSJiHwsIu0Cxh0VkcVeN7U6gz9RP+jXmkcu783c9bu57vkFHM4v9DskY4ypFZUmfRGJBB4HLgC6A2NEpOzdor4EMlX1FOBN4KGAcbmq2sfrRlZT3CftslNb8+gVfZi3YTfXPreAQ0cs8RtjQl8wR/oDgLWqul5V84EpwKjACVR1hqoe9nrnAq2rN8yacWnfVvz1h31YsHEP1z63gBxL/MaYEBdM0m8FbA7o3+INq8j1wPsB/bEikiUic0Xk0vJmEJHx3jRZ2dnZQYRUfUb1acVjo/uy8Ju9XDNpviV+Y0xIq9ard0RkLJAJ/CVgcDtVzQSuBP4mIt+5x6iqTlTVTFXNTElJqc6QgnJJ75b8fXRfvty8j6ufncfBvIJaj8EYY2pDMEl/K9AmoL+1N6wUERkG/BYYqapHioer6lbvdT0wE6iT91G96JQWPH5lX5Zs2c/Vk+ZzwBK/MSYEBZP0FwAZItJeRGKA0UCpq3BEpC/wFC7h7woYniwiDbz3zYAzgeXVFXx1G9GzBY//6FSWbtnPVc/OZ3+uJX5jTGipNOmraiFwCzAdWAG8rqrLROQeESm+GucvQCLwRplLM7sBWSLyFTADeEBV62zSBzi/R3OeGNuP5dv2c9Wz89h/2BK/MSZ0BPXnLFWdBkwrM+yugPfDKpjvc6DXyQToh+Hd03hybD9umryIHz07l8nXD6RxfIzfYRljzEkLq9swVMXQbmk8dVU/Vu/I4cqn57H3UL7fIRljzEmzpH8c53RNZeLV/VibncOVz8xjjyV+Y0w9Z0m/EkO6pPLM1Zmsz87hyqfnsjvnSOUz1QXXvmf3ATLGfIcl/SCc3TmFZ8f1Z8O3h7jy6Xl8W18SvzHGlGFJP0hnZTRj0jX92bTnEGMmziX7oCV+Y0z9Y0m/Cs7s5BL/lr25jHl6LrsO5vkdkjHGVIkl/So6o2Mznru2P1v35jJm4lx2HbDEb4ypPyzpn4DTOjTlhesGsH1/HqMnzmXHfkv8xpj6wZL+CRrQvgkvXjeAnQfyGD3xC7bvz/U7JGOMqZQl/ZOQmd6EF68fwLc5+YyeOJdt+yzxG2PqNkv6J6lfO5f493iJf6slfmNMHWZJvxqc2jaZl24YyN7D+fzwqS/YvOdw5TMZY4wPLOlXkz5tGjP5+oFs35/H0EdnMWt1Nqrqd1jGGFOKJf1q1LtNY7o1TwJg3KT5XP7kF3yxbrfPURljTImgbq1sgvfurYPILyzitazN/POTNYx5ei5ndmrK7cO70K9dst/hGWPCnB3p14CYqAizeSEkAAAUd0lEQVSuOq0ds355Dr+/uDurdhzk+098zrXPzWfplv1+h2eMCWOW9GtQbHQk15/Vntl3nMOvRnRl0Tf7uOSfn3LjS1ms3HHA7/CMMWHIkn4tiI+J4qYhHZnzq3P46bAMPl+7mwsem8P/vfol67Jz/A7PGBNGLOnXooax0fx0WGfm/OocbhrckY9X7GT4o7P4+etf8c1uu8zTGFPzLOn7oHF8DHeM6MrsO87hujPb8+6SbZz7yEzufHup/avXGFOjgkr6IjJCRFaJyFoR+XU5428XkeUiskREPhaRdgHjxonIGq8bV53B13fNEhvwu4u7M/uOc7hyYFveyNrMkL/M5O6py+y2zcaYGlFp0heRSOBx4AKgOzBGRLqXmexLIFNVTwHeBB7y5m0C/AEYCAwA/iAidt1iGWkNY7lnVE9m/GIIl53aipfmbuLsh2bw52kr7Lm8xphqFcyR/gBgraquV9V8YAowKnACVZ2hqsWV0nOB1t7784EPVXWPqu4FPgRGVE/ooad1cjwPfP8UPr59MBf2bMHEOesZ9OAnPPLBKvbnFvgd3omxZ/UaU6cEk/RbAZsD+rd4wypyPfB+VeYVkfEikiUiWdnZ2UGEFNrSmyXw6A/78OHPzmZI11T+8claznrwE/7x8RpyjhT6HZ4xph6r1oZcERkLZAJ/qcp8qjpRVTNVNTMlJaU6Q6rXOqUm8fiVpzLt1kEMbN+URz5czaAHP+GpWevIzT/qd3j1i51xGAMEl/S3Am0C+lt7w0oRkWHAb4GRqnqkKvOa4+vesiHPjMvkPzefSa/Wjfnz+ysZ9NAMnvtsA3kFlvyNMcELJukvADJEpL2IxACjgamBE4hIX+ApXMLfFTBqOnCeiCR7DbjnecPMCejTpjEvXjeANyacTseUBP743+Wc8/BMXpn3DQVHi/wOzxhTD1Sa9FW1ELgFl6xXAK+r6jIRuUdERnqT/QVIBN4QkcUiMtWbdw9wL67gWADc4w0zJ6F/ehOmjD+Nl28YSItGsdz59lLOfWQmby7cQqElf2PMcQR1l01VnQZMKzPsroD3w44z7yRg0okGaMonIpzZqRlndGzKzFXZPPLhKn7xxlf8a8Zafjq8M5O/2IiI8NqNp/sdqjGmDrFbK9dzIsI5XVMZ0iWF6ct28tcPV3Prq18SFx1J6+Q4VBUR8TtMY0wdYbdhCBEiwoiezXn/tkH8fUxfFGXNrhxueCHLbu1gjDnGkn6IiYgQRvZuySmtGtG2STyfr9vN8Edn8eIXGykqssc3GhPuLOmHKBGhRaNYPvjZ2ZzaLpm73lnG5U99wZqdB/0OzRjjI6vTD1GBDbgvXjeAtxZt5d73lnPR3z/l5nM6cdOQjsREWZlvTLixvT4MiAjf79eaj24fzIiezfnrR6u5+B9zWLhpr9+hGWNqmSX9MNIssQF/H9OXSddkkpNXyA+e/Jy7py6z+/kYE0Ys6Yehc7um8cHtgxl3ejovfLGR8x6dxScrd/odljGmFljSD1OJDaK4e2QP3pxwBgkNorju+SxuffVLvs05UvnMxph6y5J+mOvXLpl3bz2Lnw3rzPtfb2fYo7P498ItqNrlncaEIkv6hgZRkdw2LINptw6iY0oiP3/jK66eNJ/Ne+xh7caEGkv65piMtCTeuPF07h3Vg0Wb9nLeX2fzzJz1dhO3UGTPFwhblvRNKRERwlWnp/Ph7YM5o2NT7ntvBZc98TnLtx3wO7TQYMnW+MySvilXy8ZxPDMuk3+M6cvWvbmM/Oen/GX6SntoizH1nCV9UyER4ZLeLfno9sFc2rcVj89Yx4WPzWHu+t1+h2aMOUGW9E2lkhNiePjy3ky+fiAFRUWMnjiX37y1lP25BX6HZoypIkv6JmhnZTRj+k/PZvzZHXhtwTcMf3QW//t6h99hGWOqwJK+qZL4mCjuvLAb79x8Fk0TGzBh8kImvLSQnQfy/A7NGBMES/rmhPRq3Yipt5zJHSO68MmqXQx7dBavzv/G7tlvTB0XVNIXkREiskpE1orIr8sZf7aILBKRQhH5QZlxR72HpR97YLoJDdGREfxkSCem//RserRsyG/eWsqYp+eyPjvH79CMMRWoNOmLSCTwOHAB0B0YIyLdy0z2DXAN8Eo5H5Grqn28buRJxmvqoPbNEnj1x6fxwGW9WL79ACMem8PjM9ZSYH/qMqbOCeZIfwCwVlXXq2o+MAUYFTiBqm5U1SWA7eVhSkQYPaAtH98+mKFdU/nL9FVc8o9PWbJln9+hGWMCBJP0WwGbA/q3eMOCFSsiWSIyV0QurVJ0pt5JbRjLE2P78eTYfuw5lM/If35Gv3s/5LnPNvDZ2m/ZdTDPbuZmjI9q43GJ7VR1q4h0AD4RkaWqui5wAhEZD4wHaNu2bS2EZGraiJ7NOb1jU4Y/Oos9h/L543+XHxuXHB9NRloSndMS6ZyWdKxrkhDjY8Sm1tntKHwRTNLfCrQJ6G/tDQuKqm71XteLyEygL7CuzDQTgYkAmZmZdhgYIhrFRTP/t8NQVbJzjrBmZw6rdx70uhzeWbyNg3klT+1qlhhDRmoSXZonkVFcIKQm0Sg+2selMCEvzAqfYJL+AiBDRNrjkv1o4MpgPlxEkoHDqnpERJoBZwIPnWiwpn4SEVKTYklNiuXMTs2ODVdVdh44wqqdB1kTUBi8kbWZQ/kl9/hJa9iAzmlJZKR6ZwfNk8hITSQp1goDY6qq0qSvqoUicgswHYgEJqnqMhG5B8hS1aki0h94G0gGLhGRP6pqD6Ab8JSIFOHaDx5Q1eUVfJUJMyJC80axNG8Uy+DOKceGqypb9+UeOzNwhUIOr8zfRF5BybUCLRvFfqeaKCMtkfiY727WP3zqCwBeu/H0ml8wY+qwoOr0VXUaMK3MsLsC3i/AVfuUne9zoNdJxmjCjIjQOjme1snxnNM19djwoiJly95cVnlnBWu8M4Mv1u8mv7CkMGidHEeXtKRSBUJRkRIRIX4sjjF1Sm005BpTLSIihLZN42nbNJ7h3dOODT9apGzafYjVO3NcQbArh9U7DjJ7TTYFR0uaiJJio5ixahdDOqcgYgWACU+W9E29FxkhdEhJpENKIiN6Nj82vOBo0bHC4L73lvPtwXyufW4BXZsncdOQjlzUqwVRkXYnEhNebIs3ISs6MoJOqUlc2KsFbZLj6d2mEQ9f3pvCIuW2KYs555GZvDR3kz0YxoQVO9I3YSNChB/0a81lfVvx0Yqd/GvmOn7/n6957KPVXHtme8ae1o5GcXZFkPFJLV06KnXt35GZmZmalZXldxgmDKgqc9fv4YlZ65i9OpukBlH86LR2XHdmOqkNY/0Oz5gqEZGFqppZ6XSW9I2Br7fu58lZ65i2dDtRkRH8oF9rxg/qQHqzBL9DMyYolvSNOQEbvz3ExDnreTNrC4VFRVzYqwUTBnekZ6tGfodmzHFZ0jfmJOw6kMekzzYyee4mco4UcnbnFG4a3JHTOjSxyz1NnWRJ35hqsD+3gMlzN/HcZxv4NiefPm0ac9OQjgzvlmZ/9jJ1iiV9Y6pRXsFR3li4hYmz17F5Ty4dUxKYMLgjo/q0IibKrnw2/rOkb0wNKDxaxHtLt/PEzHWs3HGQlo1iuWFQB0YPaFPuPX+MqS2W9I2pQarKzNXZPDFjHfM37qFxfDTXnJHOuNPTSbbnAhgfWNI3ppZkbdzDk7PW8dGKXcRFRzJmQFtuGNSelo3j/A7NhBFL+sbUslU7DvLUrHW889U2BLi0bysmDO5Ap9Qkv0MzYcCSvjE+2bL3MM/M2cCUBd9wpLCI87qnMWFwR/q2TfY7NBPCLOkb47PdOUd44fONPP/5Rg7kFXJ6h6ZMGNKRge2bEBsd6Xd4JsRY0jemjsg5Usir877hmU/Xs/PAEQBioiJoGBtNw7gokmKjaRgbRcO46GPDGlY0LC6apNgo4qIjT+pPYvYksdATbNK3a8yMqWGJDaL48dkduPqMdvzv6x1s2ZvLgbwCDuQWcjCvgAN5hRzILWDrvlwO5BZyIK+g1JPAyhMVIV6BUFIwJMVGlVtAFL8PHK6q9s/iMGVJ35ha0iAqklF9WgU1bV7BUQ7mFXqFgysYDnoFRcmwAjeNN37ngbxjww7nH/8ZAQIkxkbxzJz1DO2WRnu7sVzYsOodY0JQwdGiYwVC6cLDFRzPfrqB/bkF5HoPkOmQksCwbmmc2zWVzHbJ9kSxeqhaq3dEZATwGBAJPKOqD5QZfzbwN+AUYLSqvhkwbhzwO6/3PlV9IbhFMMacqOjICJokxNCkgj+KfbRiJwAPX96bj1fs5OOVu3jusw1MnL2eRnHRDOmSwtBuaQzOSKFRvD1YJpRUeqQvIpHAamA4sAVYAIxR1eUB06QDDYFfAFOLk76INAGygExAgYVAP1XdW9H32ZG+Mf7IOVLInNXZfLRiFzNW7WLPoXwiI4T+6ckM65Zm1UB1XHUe6Q8A1qrqeu+DpwCjgGNJX1U3euPKtj6dD3yoqnu88R8CI4BXg/heY0wtSmwQxQW9WnBBrxYcLVIWb97nzgJW7OK+91Zw33sr6NAsgaHdUhnaLc2qgeqpYJJ+K2BzQP8WYGCQn1/evMG1ZBljfBMZIfRrl0y/dsncMaIrm/cc5pOVu/hoxU6e/3wjT8/ZcKwa6NyuqQzpnGrVQPVEnbh6R0TGA+MB2rZt63M0xpiy2jSJZ9wZ6Yw7I/1YNdDHK3cxY+Uu3lm8rVQ10LldU+mQkuh3yKYCwST9rUCbgP7W3rBgbAWGlJl3ZtmJVHUiMBFcnX6Qn22M8UFVqoHO7ZpG/3SrBqpLgmnIjcI15A7FJfEFwJWquqycaZ8H3i3TkLsQONWbZBGuIXdPRd9nDbnG1F+B1UDz1u8h/2gRDWOjGNIllaHdrBqoJlXrbRhE5ELcJZmRwCRVvV9E7gGyVHWqiPQH3gaSgTxgh6r28Oa9DrjT+6j7VfW5432XJX1jQkPOkUI+XeNdDbRyF7u9q4Ey2xVfDWTVQNXJ7r1jjKkzAquBPlm5i5U7DgLQICqCXq0akZwQQ5P4GBonRNMkPobk+Bg3LCGaxvFuXKO4aHsu8XFY0jfG1Fmb9xxm7LPzOJhXSNfmSew5lM/ew/nsPVRA/tHy7zsUIdAoLrqkgIh3hcKxAiI+hsbx0TRJcP3JXkERGSYFhd1wzRhTZ7VpEs+sX57zneGqyuH8o+w5lM++wwXsOZzPvsP5rlA4lM9eb9jeQ/ls3ZfL11v3s+dwfoU3qBOvoGgSX1wQuEKiSUJJofHMnA3EREUwZfxpJMWGfnuDJX1jTJ0hIiQ0iCKhQRRtmgQ3j6qSWxBQUBw7a8hnz+GCkkLjcD7b9uWxbNsB9hzK50iZgqLX3R/QvGEsGWmJdExJJCMtkYzUJDqlJlZ4O4v6yJK+MaZeExHiY6KIj4midRUeTpabf5Q9h/O58cUs8guL+N6prVmz6yDrduXwetbmUncqbZoQQ6dUVxB0SkkkIy2JjNREUpIa1LtbVFvSN8aEpbiYSFrFxPHurYO+M66oSNl+II81Ow+ydlcOa3flsGZXDlMXb+NAXuGx6ZJio8hILTkj6JSWSEZqIi0bxdXZRmdL+sYYU0ZEhNCqcRytGscxpEvqseGqSnbOEdbudIWAKwwO8vHKnbyWVXLHmbjoSHdmkJpIR+81Iy2JNslxvv9RzZK+McYESURITYolNSmWMzo1KzVu76F81mbnsGanKwjW7srhi/W7eevLkhsYxERG0CElwZ0VeGcIGWmJpDdNICaqdgoDS/rGGFMNkhNi6J/QhP7ppVugD+YVsC77UKmqoiVb9vPe0u0UXzEfGSFERwoNY6OZ/9thNRqnJX1jjKlBSbHR9GnTmD5tGpcanpt/lHXZOazzzg5enrepVuKxpG+MMT6Ii4mkZ6tG9GzVCIBfnN+lVr7Xbn1njDFhxJK+McaEEUv6xhgTRizpG2NMGLGkb4wxYcSSvjHGhBFL+sYYE0Ys6RtjTBixpG+MMWGkzj0uUUSygZP5P3Iz4NtqCqe+s3VRmq2P0mx9lAiFddFOVVMqm6jOJf2TJSJZwTwnMhzYuijN1kdptj5KhNO6sOodY4wJI5b0jTEmjIRi0p/odwB1iK2L0mx9lGbro0TYrIuQq9M3xhhTsVA80jfGGFMBS/rGGBNGQibpi8gIEVklImtF5Nd+x+MnEWkjIjNEZLmILBOR2/yOyW8iEikiX4rIu37H4jcRaSwib4rIShFZISKn+x2Tn0TkZ95+8rWIvCoisX7HVJNCIumLSCTwOHAB0B0YIyLd/Y3KV4XAz1W1O3AacHOYrw+A24AVfgdRRzwG/E9VuwK9CeP1IiKtgFuBTFXtCUQCo/2NqmaFRNIHBgBrVXW9quYDU4BRPsfkG1XdrqqLvPcHcTt1K3+j8o+ItAYuAp7xOxa/iUgj4GzgWQBVzVfVff5G5bsoIE5EooB4YJvP8dSoUEn6rYDNAf1bCOMkF0hE0oG+wDx/I/HV34A7gCK/A6kD2gPZwHNeddczIpLgd1B+UdWtwMPAN8B2YL+qfuBvVDUrVJK+KYeIJAL/Bn6qqgf8jscPInIxsEtVF/odSx0RBZwKPKGqfYFDQNi2gYlIMq5WoD3QEkgQkbH+RlWzQiXpbwXaBPS39oaFLRGJxiX8l1X1Lb/j8dGZwEgR2Yir9jtXRCb7G5KvtgBbVLX4zO9NXCEQroYBG1Q1W1ULgLeAM3yOqUaFStJfAGSISHsRicE1xEz1OSbfiIjg6mxXqOqjfsfjJ1X9jaq2VtV03HbxiaqG9JHc8ajqDmCziHTxBg0FlvsYkt++AU4TkXhvvxlKiDdsR/kdQHVQ1UIRuQWYjmt9n6Sqy3wOy09nAlcBS0VksTfsTlWd5mNMpu74P+Bl7wBpPXCtz/H4RlXnicibwCLcVW9fEuK3ZLDbMBhjTBgJleodY4wxQbCkb4wxYcSSvjHGhBFL+sYYE0Ys6RtjTBixpG+MMWHEkr4xxoSR/wdFCK5wLYhmWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x166ecaba6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9\n"
     ]
    }
   ],
   "source": [
    "# n_components = [20, 30, 40]\n",
    "# learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05] # 10] # 0.01 - 1\n",
    "\n",
    "n_components = [40]\n",
    "learning_rates = [11]\n",
    "n_epoches = 10\n",
    "for n_comp in n_components:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "#         pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"softmax\")\n",
    "        pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"logistic\")\n",
    "        pipeline.build(n_components=n_comp, learning_rate=lr, n_epoches=n_epoches, batch_size=None, n_repeats=1)\n",
    "        pipeline.run()\n",
    "        \n",
    "        # plot errors\n",
    "        pipeline.records.plt_losses(n_components=n_comp, lr=lr, n_epoches=n_epoches)\n",
    "        pipeline.records.show_accuracies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
