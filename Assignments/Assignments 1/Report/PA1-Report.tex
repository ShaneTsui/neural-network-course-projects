\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{enumerate}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Report of classification task on CAFE Dataset}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}
conclusions section 

\subsection{Introduction}

In this report, we focus on a set of classification tasks on California Facial Expresion(CAFE) dataset.
Two tasks are solved, which are

\begin{itemize}
    \item Two face classification: Use logistic regression to separate 2 kinds of facial expressions
    \item Six face classification: Use softmax regression to classify 6 emotional faces
\end{itemize}

\section{Methods}
In this section, we would show the basic pipeline of two task solutions.


\subsection{Test methods and dataset building}

In order to test the robustness of the algorithm and parameter selection, we test over all the subjects by making each one
as testee once. And report the averaged 10 accuracies over the 10 runs.

Since different task require various facial expressions, we decide to build data in a dynamical way.
This shall follow these two steps
\begin{itemize}
    \item Screen out useless data: Filter out the facial expressions labeled by 'neutral' and 'happy'.
    \item Split data: Divide the dataset in terms of 10 subjects with ratio of 80\%, 10\%, 10\% each for training, holdout and test dataset, respectively.
    \item Facial ecpression selection: Select task-related facial expressions according to the requirements
\end{itemize}

Specifically, in the second step, we iterate through all the 10 subjects and build test dataset from each subject exactly once.
The holdout dataset is selected from the rest of the subjects, and all the other 8 subjects are used as training data.

Thus, we would have 10 randomly built dataset with different testee appeared exactly once.

To be specific, for each selection of testee, the numerical features of dataset constitution should be as in Table 1.

\begin{table}[h]
    \caption{Dataset constitution}
    \label{evaluation-of-f}
    \begin{center}
    \begin{tabular}{lll}
    \multicolumn{1}{c}{\bf Train}  &\multicolumn{1}{c}{\bf Holdout} &\multicolumn{1}{c}{\bf Test}
    \\ \hline \\
    8 subjects & 1 subject & 1 subject \\
    48 images & 6 images & 6 images \\
    \end{tabular}
    \end{center}
    \end{table}

\subsection{Data preprocessing}
After loading all the images, we carry out following pre-processing methods.
\begin{itemize}
    \item Centerize data: Calculate the mean over training dataset, and subtract this value from training, holdout and test dataset to get centerized data.
    \item PCA: Fit PCA model with centerized training data, and perform transformation(compression) over training, holdout and test dataset.
\end{itemize}

We describe some detailed implementation in PCA.

\begin{enumerate}[1]
    \item Avoid high dimentional matrix calculation
    
    By following the instruction in \href{https://stackoverflow.com/questions/13224362/principal-component-analysis-pca-in-python}{this article}, we avoided the calculation of a matrix of size 91200 by 91200 
    
    \item Each principle vector is scaled by the standard diviation.
    \item Only use the training data to compute the principle components
    
    Reason: To prevent overfitting, which means the model performs very well on training dataset but terrible on testing data. The test and validation data shouold only be used to measure the performance of the model trained on train dataset. 
    If the test or holdout dataset are exposed to PCA algorithm, the final eigen vectors would have 'learned' from test / holdout, which would cause the performance 
    to be falsely biased on validation and test dataset.
\end{enumerate}

\subsection{Two face logistic classification}
We used logistic classification over 2 sets of facial expressions.

\begin{itemize}
    \item Happy VS Maudlin
    \item Afraid VS Surprised
\end{itemize}

We performed SGD and batch gradient descent on both datasets. Here's the parameters we used
$$learning\ rate = $$
$$epoches = 10$$

\subsection{Six face softmax classification}

We performed SGD and batch gradient descent on the dataset. Here's the parameters we used
$$learning\ rate = $$
$$epoches = 10$$

\section{Results}
In this part, we report the required statistics over the two tasks described above.

\subsection{Two face logistic classification}

\subsection{Six face softmax classification}


\end{document}
