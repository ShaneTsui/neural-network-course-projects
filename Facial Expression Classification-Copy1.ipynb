{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pre-process data\n",
    "\n",
    "## What's PCA\n",
    "对一个nxn的对称矩阵进行分解，求出它的特征值和特征向量 产生n个n维的正交基，每个正交基会对应一个特征值。\n",
    "把矩阵投影到这N个基上，此时特征值的模就表示矩阵在该基的投影长度。特征值越大，说明矩阵在对应的特征向量上的方差越大，样本点越离散，越容易区分，信息量也就越多。因此，特征值最大的对应的特征向量方向上所包含的信息量就越多，如果某几个特征值很小，那么就说明在该方向的信息量非常少，我们就可以删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用的信息量都保留下来了。\n",
    "\n",
    "PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。\n",
    "最后只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。\n",
    "\n",
    "将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大\n",
    "\n",
    "解释\n",
    "\n",
    "(1)最大方差理论；(2)最小化降维造成的损失\n",
    "在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。\n",
    "\n",
    "## How to\n",
    "计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择**特征值最大(即方差最大)的k个特征所对应的特征向量**组成的矩阵\n",
    "\n",
    "得到协方差矩阵的特征值特征向量有两种方法:\n",
    "1. 特征值分解协方差矩阵\n",
    "2. 奇异值分解协方差矩阵, SVD(Singular Value Decomposition)\n",
    "\n",
    "设有n条m维数据\n",
    "1. 将原始数据按列组成m行n列矩阵X\n",
    "2. 将X的每一行(代表一个属性字段）进行零均值化\n",
    "3. 求出协方差矩阵 \n",
    "4. 求出协方差矩阵的特征值及对应的特征向量\n",
    "5. 将特征相浪按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P\n",
    "6. Y=PX即为降维到k维后的数据\n",
    "\n",
    "## Calculate PCA\n",
    "对角化协方差矩阵\n",
    "1. Calculating the covariance matrix;\n",
    "2. Taking the eivenvectors & eigenvalues of this cov matrix\n",
    "\n",
    "\n",
    "特征值 => PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure code for dataset building  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_image_dict= {}\n",
    "    \n",
    "    def add(self, image, label):\n",
    "        self.label_image_dict[label] = image\n",
    "    \n",
    "    def get(self, label):\n",
    "        return self.label_image_dict[label]\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, data=[], labels=[]):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def to_numpy_array(self):\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "    def insert(self, datum, label):\n",
    "        self.data.append(datum)\n",
    "        self.labels.append(label)\n",
    "    \n",
    "    def extend(self, data, labels):\n",
    "        self.data.extend(data)\n",
    "        self.labels.extend(labels)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        idx = np.array(list(range(len(self.data))))\n",
    "        np.random.shuffle(idx)\n",
    "        self.data[:] = self.data[idx]\n",
    "        self.labels[:] = self.labels[idx]\n",
    "    \n",
    "class DataBuilder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.subjects = defaultdict(Subject)\n",
    "    \n",
    "    def get_subject_ids(self):\n",
    "        return list(self.subjects.keys())\n",
    "    \n",
    "    def load_data(self, data_dir=\"./CAFE/\"):\n",
    "        # Get the list of image file names\n",
    "        all_files = listdir(data_dir)\n",
    "        \n",
    "        # Store the images and labels in self.subjects dictionary\n",
    "        for file in all_files:\n",
    "            # Load in the files as PIL images and convert to NumPy arrays\n",
    "            subject, rest_string = file.split('_')\n",
    "            label = rest_string.split('.')[0][:-1]\n",
    "            \n",
    "            # Exclude neutral and happy faces \n",
    "            if label != 'n' and label != 'h':\n",
    "                img = Image.open(data_dir + file)\n",
    "                self.subjects[subject].add(np.array(img, dtype=np.float64).reshape(-1, ), label) # Reshaped to a vector        \n",
    "        \n",
    "    def build_dataset(self, test_subject_id, labels):\n",
    "        train, holdout, test = Dataset(), Dataset(), Dataset()\n",
    "        \n",
    "        # Select data for train, holdout and test dataset\n",
    "        subject_ids = self.get_subject_ids()\n",
    "        test_subject = self.subjects[test_subject_id]\n",
    "        subject_ids.remove(test_subject_id)\n",
    "        \n",
    "        holdout_subject_id = random.choice(subject_ids)\n",
    "        holdout_subject = self.subjects[holdout_subject_id]\n",
    "        subject_ids.remove(holdout_subject_id)\n",
    "        \n",
    "        for label in labels:\n",
    "            test.insert(test_subject.get(label), label)\n",
    "            holdout.insert(holdout_subject.get(label), label)\n",
    "            train.extend([self.subjects[train_subject_id].get(label) for train_subject_id in subject_ids], [label] * len(subject_ids))\n",
    "            \n",
    "#         # Select data for PCA\n",
    "#         for train_subject_id in subject_ids:\n",
    "#             pca.extend(list(self.subjects[train_subject_id].label_image_dict.values()))\n",
    "        \n",
    "        # To numpy array\n",
    "        train.to_numpy_array()\n",
    "        holdout.to_numpy_array()\n",
    "        test.to_numpy_array()\n",
    "        \n",
    "        return train, holdout, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_face(img):\n",
    "    \"\"\" Display the input image and optionally save as a PNG.\n",
    "    Args:\n",
    "        img: The NumPy array or image to display\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    img = img.reshape(380, 240)\n",
    "    # Convert img to PIL Image object (if it's an ndarray)\n",
    "    if type(img) == np.ndarray:\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(np.asarray(img), cmap='gray') # for jupyter notebook inline display\n",
    "    plt.axis('off')\n",
    "        \n",
    "def display_faces(images, layout, labels):\n",
    "    (n_row, n_col) = layout\n",
    "    assert n_row*n_col == len(images) == len(labels)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)  # 1-6\n",
    "        plt.title(\"{} face\".format(labels[i]))\n",
    "        display_face(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PCA:\n",
    "    \n",
    "#     def __init__(self, data):\n",
    "#         \"\"\"\n",
    "#         data is stored row wise\n",
    "#         \"\"\"\n",
    "#         self.data = data\n",
    "#         self.eig_val, self.eig_vec = self.__pca__(data)\n",
    "    \n",
    "#     def __pca__(self, data):\n",
    "#         \"\"\"\n",
    "#         returns: data transformed in 2 dims/columns + regenerated original data\n",
    "#         pass in: data as 2D NumPy array\n",
    "#         \"\"\" \n",
    "        \n",
    "#         data -= data.mean(axis=0)\n",
    "\n",
    "#         # Calculate covariance\n",
    "#         cov = np.dot(data, data.T)\n",
    "\n",
    "#         eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        \n",
    "#         # Map the eigenvectors to original ones\n",
    "#         eig_vecs = np.dot(data.T, eig_vecs)\n",
    "        \n",
    "#         # Normalization\n",
    "#         eig_vecs = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "# #         print(np.dot(data, eig_vecs[:,1:])/np.sqrt(eig_vals[1:]))\n",
    "# #         print(np.var(np.dot(data, eig_vecs)))\n",
    "# #         print(eig_vals)\n",
    "#         return eig_vals[1:], eig_vecs[:, 1:]\n",
    "    \n",
    "#     def transform(self, data, n_components):\n",
    "# #         selected_idx = self.sorted_idx[:n_components]\n",
    "#         tran_data = np.dot(data, self.eig_vec[:, -n_components:])\n",
    "# #         print(\"tran\")\n",
    "# #         print(tran_data / np.sqrt(self.eig_val[-n_components:]))\n",
    "#         return tran_data / np.sqrt(self.eig_val[-n_components:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22222222 0.66666667 1.55555556 0.66666667]\n",
      "[1.00673929e-15 2.90283246e+00 6.43050087e+00]\n",
      "[1.39549585e-16 7.27667955e-16 2.90283246e+00 6.43050087e+00]\n",
      "[[-0.6561384   0.67995555  0.07953533 -0.31751691]\n",
      " [-0.38082017 -0.42000544  0.8185381   0.09255701]\n",
      " [-0.12158053 -0.48499296 -0.21043072 -0.84007078]\n",
      " [ 0.64005982  0.35501791  0.52857205 -0.42999686]]\n",
      "[[ 0.07953533 -0.31751691]\n",
      " [ 0.8185381   0.09255701]\n",
      " [-0.21043072 -0.84007078]\n",
      " [ 0.52857205 -0.42999686]]\n",
      "[[ 2.90283246e+00 -1.49261422e-15]\n",
      " [-1.49261422e-15  6.43050087e+00]]\n",
      "[0.96761082 2.14350029]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,4,5],[2,3,6,7],[1,4,3,6]])\n",
    "print(np.var(x, axis=0))\n",
    "y = x-x.mean(axis=0)\n",
    "z  = np.dot(y, y.T)\n",
    "v, w = np.linalg.eigh(z)\n",
    "print(v)\n",
    "u = np.dot(y.T,y)\n",
    "g,h =  np.linalg.eigh(u)\n",
    "print(g)\n",
    "# print(np.diag(u))\n",
    "print(h)\n",
    "r = np.dot(y.T, w)\n",
    "r = r / np.linalg.norm(r, 2, axis=0)\n",
    "r = r[:,1:]\n",
    "print(r)\n",
    "new_y = np.dot(y, r)\n",
    "print(np.dot(new_y.T, new_y))\n",
    "print(np.var(new_y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def learn(self, raw):\n",
    "        # Subtract the mean\n",
    "        self.mu = raw.mean(axis=0)\n",
    "        raw -= self.mu\n",
    "\n",
    "        # Calculate covariance\n",
    "        cov = np.dot(raw, raw.T)\n",
    "        eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "        self.std = np.sqrt(eig_vals)[1:]\n",
    "\n",
    "        # Map the eigenvectors to original ones\n",
    "        eig_vecs = np.dot(raw.T, eig_vecs)[:, 1:]\n",
    "\n",
    "        # Normalization\n",
    "        self.transformer = eig_vecs / np.linalg.norm(eig_vecs, 2, axis=0)\n",
    "        \n",
    "    def run(self, data, n_components, normalize=True):\n",
    "        data = data - self.mu\n",
    "        data = np.dot(data, self.transformer[:, -n_components:])\n",
    "        if normalize:\n",
    "            data = data / self.std[-n_components:]\n",
    "        return data\n",
    "    \n",
    "    def plt_eig_faces(self, n_faces=6, layout=(2,3)):\n",
    "        # display eigenfaces\n",
    "        display_faces(self.transformer.T[0:n_faces], layout=layout, \n",
    "                      labels=[\"eigenface {}\".format(i) for i in range(n_faces)])\n",
    "        plt.savefig('Eigenfaces.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display 6 different emotional faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1(c) Display eigen face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.train_set, self.holdout_set, self.test_set = None, None, None\n",
    "        self.w = None\n",
    "        \n",
    "        self.test_accuracies = []\n",
    "    \n",
    "    def load_data(self, train, holdout, test):\n",
    "        self.train_set, self.holdout_set, self.test_set = train, holdout, test\n",
    "        self.encode = self.encoder(train.labels)\n",
    "        \n",
    "        for dataset in [self.train_set, self.holdout_set, self.test_set]:\n",
    "            dataset.X = self.bias(dataset.data)\n",
    "            dataset.y = self.encode(dataset.labels)\n",
    "        \n",
    "    def bias(self, data):\n",
    "        return np.column_stack((np.ones(len(data)), data))\n",
    "\n",
    "    def accuracy(self, test_set=None):\n",
    "        if not test_set:\n",
    "            test_set = self.test_set\n",
    "#         y = np.argmax(y, axis=1)[:, np.newaxis]\n",
    "        return np.sum(self.predict(test_set.X) == test_set.y) / len(test_set.y)\n",
    "    \n",
    "    def test(self):\n",
    "        self.test_accuracies.append(self.accuracy())\n",
    "        \n",
    "    def train(self, T=10, lr=0.06, bs=None):\n",
    "        \n",
    "        # initialization\n",
    "        train_X, train_y = self.train_set.X, self.train_set.y\n",
    "        holdout_X, holdout_y = self.holdout_set.X, self.holdout_set.y\n",
    "        self.train_losses, self.holdout_losses = [], []\n",
    "        self.w = np.zeros((train_X.shape[1], train_y.shape[1]))\n",
    "        self.W = []\n",
    "#         self.W = np.array([])\n",
    "\n",
    "        # gradient descent\n",
    "        for t in range(T):\n",
    "            # gradient descent on each batch\n",
    "            if bs:\n",
    "                # generate random permutation\n",
    "                perm = np.random.permutation(len(train_X))\n",
    "                for i in range(round(len(train_X)/bs)):\n",
    "                    train_X_batch, train_y_batch = train_X[perm[i:i+bs]], train_y[perm[i:i+bs]]\n",
    "                    self.w -= lr * self.gradient(train_X_batch, train_y_batch)\n",
    "            else:\n",
    "                self.w -= lr * self.gradient(train_X, train_y)\n",
    "#                 print(\"temporary w: {}\".format(self.w))\n",
    "            self.W.append(self.w.tolist())\n",
    "#             self.W = np.append(self.W, self.w)\n",
    "#             print(\"temporary W: {}\".format(self.W))\n",
    "            \n",
    "            # compute losses on train dataset and holdout dataset\n",
    "            print(\"train losses\")\n",
    "            print(self.loss(train_X, train_y))\n",
    "            self.train_losses.append(self.loss(train_X, train_y))\n",
    "            self.holdout_losses.append(self.loss(holdout_X, holdout_y))\n",
    "            \n",
    "        # save the parameters with best performance\n",
    "#         print(\"The W: {}\".format(np.array(self.W)))\n",
    "        self.w = np.array(min(self.W, key=lambda w: self.holdout_losses[self.W.index(w)]))\n",
    "#         print(self.holdout_losses)\n",
    "#         print(self.W)\n",
    "#         print(np.array(self.W).shape)\n",
    "#         print(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        assert len(label_set) == 2\n",
    "        binary_dict = {label_set[0]:0, label_set[1]:1}\n",
    "        return lambda labels: np.array([binary_dict[label] for label in labels]).reshape(-1, 1)\n",
    "    \n",
    "    def logistic(self, s):\n",
    "        return np.array(1 / (1 + np.exp(-s)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.logistic(np.dot(X, self.w))\n",
    "        return np.array([1 for prob in probs if prob>0.5])\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        print(\"logistic losses: {}\".format(- np.dot(y.T, np.log(y_hat)) / len(y_hat)))\n",
    "#         return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "        return (- np.dot(y.T, np.log(y_hat)) / len(y_hat))[0][0]\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.logistic(np.dot(X, self.w))\n",
    "        return np.sum((y_hat - y) * X, axis=0).reshape(-1, 1) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(Classifier):\n",
    "    \n",
    "    def encoder(self, labels):\n",
    "        label_set = list(set(labels))\n",
    "        one_hot_dict = {label:one_hot for label, one_hot in zip(label_set, np.eye(len(label_set)))}\n",
    "        return lambda labels: np.array([one_hot_dict[label] for label in labels])\n",
    "    \n",
    "    def softmax(self, s):\n",
    "        return np.exp(s) / np.sum(np.exp(s), axis=1, keepdims=True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.softmax(np.dot(X, self.w))\n",
    "        return np.argmax(probs, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        print(\"softmax losses: {}\".format(- np.sum(y * np.log(y_hat)) / len(y_hat)))\n",
    "        return - np.sum(y * np.log(y_hat)) / len(y_hat)\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        y_hat = self.softmax(np.dot(X, self.w))\n",
    "        return np.dot(X.T, (y_hat - y)) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Records:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.holdout_losses = []\n",
    "        self.test_accuracies = []\n",
    "                \n",
    "    def record(self, train_los, holdout_los, test_acc):\n",
    "        self.train_losses.append(train_los)\n",
    "        self.holdout_losses.append(holdout_los)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        \n",
    "    def plt_losses(self, n_components, lr, n_epoches, train=True, holdout=True):\n",
    "        assert len(self.train_losses) == len(self.holdout_losses)\n",
    "        plt.figure()\n",
    "        if train:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.train_losses, axis=0), yerr=np.std(self.train_losses, axis=0))\n",
    "        if holdout:\n",
    "            plt.errorbar(range(n_epoches), np.mean(self.holdout_losses, axis=0), yerr=np.std(self.holdout_losses, axis=0))\n",
    "        plt.title(\"n_components={}, learning_rates={}, n_epoches={}\".format(n_components, lr, n_epoches))\n",
    "        plt.show()\n",
    "        \n",
    "    def show_accuracies(self):\n",
    "        print(\"The accuracy is {}\".format(np.mean(self.test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, facial_expressions, dataset_dir='./CAFE/', classifier_type=\"logistic\", pca=True):\n",
    "        self.facial_expressions = facial_expressions\n",
    "        \n",
    "        self.data_builder = DataBuilder()\n",
    "        self.data_builder.load_data(dataset_dir)\n",
    "        \n",
    "        if classifier_type==\"logistic\":\n",
    "            self.classifier = LogisticClassifier()\n",
    "        elif classifier_type==\"softmax\":\n",
    "            self.classifier = SoftmaxClassifier()\n",
    "            \n",
    "        if pca:\n",
    "            self.PCA = PCA()\n",
    "            \n",
    "        self.records = Records()\n",
    "        \n",
    "    def build(self, n_components, learning_rate, n_epoches, batch_size=None, n_repeats=10):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoches = n_epoches\n",
    "        self.batch_size = batch_size\n",
    "        self.n_repeats = n_repeats\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        subjects = self.data_builder.get_subject_ids()\n",
    "        print(\"subject: {}\".format(subjects))\n",
    "        for repeat in range(self.n_repeats):\n",
    "            for test_subject in subjects:\n",
    "                # Dataset building\n",
    "                train, holdout, test = self.data_builder.build_dataset(test_subject, self.facial_expressions)\n",
    "                self.PCA.learn(train.data)\n",
    "                \n",
    "                train.data = self.PCA.run(train.data, self.n_components)\n",
    "#                 print(train.data)\n",
    "                holdout.data = self.PCA.run(holdout.data, self.n_components)\n",
    "                test.data = self.PCA.run(test.data, self.n_components)\n",
    "\n",
    "                # Run classification algorithm\n",
    "                self.classifier.load_data(train, holdout, test)\n",
    "                self.classifier.train(lr=self.learning_rate, T=self.n_epoches, bs=self.batch_size)\n",
    "                self.classifier.test()\n",
    "                self.records.record(self.classifier.train_losses, self.classifier.holdout_losses, self.classifier.test_accuracies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: ['018', '027', '036', '037', '041', '043', '044', '048ng', '049', '050']\n",
      "train losses\n",
      "logistic losses: [[0.34795597]]\n",
      "0.34795596626665387\n",
      "logistic losses: [[0.34795597]]\n",
      "logistic losses: [[0.34643822]]\n",
      "train losses\n",
      "logistic losses: [[0.34910802]]\n",
      "0.3491080173938437\n",
      "logistic losses: [[0.34910802]]\n",
      "logistic losses: [[0.34625265]]\n",
      "train losses\n",
      "logistic losses: [[0.35005863]]\n",
      "0.3500586322643029\n",
      "logistic losses: [[0.35005863]]\n",
      "logistic losses: [[0.34602351]]\n",
      "train losses\n",
      "logistic losses: [[0.35083324]]\n",
      "0.3508332416333375\n",
      "logistic losses: [[0.35083324]]\n",
      "logistic losses: [[0.34575656]]\n",
      "train losses\n",
      "logistic losses: [[0.35145419]]\n",
      "0.35145419222618735\n",
      "logistic losses: [[0.35145419]]\n",
      "logistic losses: [[0.34545682]]\n",
      "train losses\n",
      "logistic losses: [[0.35194109]]\n",
      "0.35194109116551425\n",
      "logistic losses: [[0.35194109]]\n",
      "logistic losses: [[0.34512867]]\n",
      "train losses\n",
      "logistic losses: [[0.35231112]]\n",
      "0.3523111199343563\n",
      "logistic losses: [[0.35231112]]\n",
      "logistic losses: [[0.34477591]]\n",
      "train losses\n",
      "logistic losses: [[0.35257932]]\n",
      "0.3525793181973906\n",
      "logistic losses: [[0.35257932]]\n",
      "logistic losses: [[0.34440187]]\n",
      "train losses\n",
      "logistic losses: [[0.35275884]]\n",
      "0.3527588386930972\n",
      "logistic losses: [[0.35275884]]\n",
      "logistic losses: [[0.34400942]]\n",
      "train losses\n",
      "logistic losses: [[0.35286117]]\n",
      "0.35286117494254154\n",
      "logistic losses: [[0.35286117]]\n",
      "logistic losses: [[0.34360108]]\n",
      "train losses\n",
      "logistic losses: [[0.34795597]]\n",
      "0.34795596626665387\n",
      "logistic losses: [[0.34795597]]\n",
      "logistic losses: [[0.34618679]]\n",
      "train losses\n",
      "logistic losses: [[0.34910802]]\n",
      "0.3491080173938437\n",
      "logistic losses: [[0.34910802]]\n",
      "logistic losses: [[0.34580421]]\n",
      "train losses\n",
      "logistic losses: [[0.35005863]]\n",
      "0.3500586322643029\n",
      "logistic losses: [[0.35005863]]\n",
      "logistic losses: [[0.34542534]]\n",
      "train losses\n",
      "logistic losses: [[0.35083324]]\n",
      "0.3508332416333375\n",
      "logistic losses: [[0.35083324]]\n",
      "logistic losses: [[0.34504975]]\n",
      "train losses\n",
      "logistic losses: [[0.35145419]]\n",
      "0.35145419222618735\n",
      "logistic losses: [[0.35145419]]\n",
      "logistic losses: [[0.34467706]]\n",
      "train losses\n",
      "logistic losses: [[0.35194109]]\n",
      "0.35194109116551425\n",
      "logistic losses: [[0.35194109]]\n",
      "logistic losses: [[0.34430695]]\n",
      "train losses\n",
      "logistic losses: [[0.35231112]]\n",
      "0.3523111199343563\n",
      "logistic losses: [[0.35231112]]\n",
      "logistic losses: [[0.34393913]]\n",
      "train losses\n",
      "logistic losses: [[0.35257932]]\n",
      "0.3525793181973906\n",
      "logistic losses: [[0.35257932]]\n",
      "logistic losses: [[0.34357335]]\n",
      "train losses\n",
      "logistic losses: [[0.35275884]]\n",
      "0.3527588386930972\n",
      "logistic losses: [[0.35275884]]\n",
      "logistic losses: [[0.34320939]]\n",
      "train losses\n",
      "logistic losses: [[0.35286117]]\n",
      "0.35286117494254154\n",
      "logistic losses: [[0.35286117]]\n",
      "logistic losses: [[0.34284706]]\n",
      "train losses\n",
      "logistic losses: [[0.3476829]]\n",
      "0.3476828985540754\n",
      "logistic losses: [[0.3476829]]\n",
      "logistic losses: [[0.34613193]]\n",
      "train losses\n",
      "logistic losses: [[0.3486349]]\n",
      "0.3486349001015612\n",
      "logistic losses: [[0.3486349]]\n",
      "logistic losses: [[0.34567659]]\n",
      "train losses\n",
      "logistic losses: [[0.3494454]]\n",
      "0.349445402338352\n",
      "logistic losses: [[0.3494454]]\n",
      "logistic losses: [[0.34520908]]\n",
      "train losses\n",
      "logistic losses: [[0.35012871]]\n",
      "0.35012871044806615\n",
      "logistic losses: [[0.35012871]]\n",
      "logistic losses: [[0.34473081]]\n",
      "train losses\n",
      "logistic losses: [[0.35069775]]\n",
      "0.350697753308148\n",
      "logistic losses: [[0.35069775]]\n",
      "logistic losses: [[0.344243]]\n",
      "train losses\n",
      "logistic losses: [[0.3511642]]\n",
      "0.35116420269172566\n",
      "logistic losses: [[0.3511642]]\n",
      "logistic losses: [[0.34374676]]\n",
      "train losses\n",
      "logistic losses: [[0.35153859]]\n",
      "0.3515385851069798\n",
      "logistic losses: [[0.35153859]]\n",
      "logistic losses: [[0.34324308]]\n",
      "train losses\n",
      "logistic losses: [[0.35183039]]\n",
      "0.35183038598428706\n",
      "logistic losses: [[0.35183039]]\n",
      "logistic losses: [[0.34273285]]\n",
      "train losses\n",
      "logistic losses: [[0.35204815]]\n",
      "0.3520481461737962\n",
      "logistic losses: [[0.35204815]]\n",
      "logistic losses: [[0.34221685]]\n",
      "train losses\n",
      "logistic losses: [[0.35219955]]\n",
      "0.35219955089509064\n",
      "logistic losses: [[0.35219955]]\n",
      "logistic losses: [[0.34169581]]\n",
      "train losses\n",
      "logistic losses: [[0.34747615]]\n",
      "0.34747615000907894\n",
      "logistic losses: [[0.34747615]]\n",
      "logistic losses: [[0.34639897]]\n",
      "train losses\n",
      "logistic losses: [[0.34824971]]\n",
      "0.34824971085489487\n",
      "logistic losses: [[0.34824971]]\n",
      "logistic losses: [[0.34620055]]\n",
      "train losses\n",
      "logistic losses: [[0.34890651]]\n",
      "0.3489065148333924\n",
      "logistic losses: [[0.34890651]]\n",
      "logistic losses: [[0.34598071]]\n",
      "train losses\n",
      "logistic losses: [[0.3494577]]\n",
      "0.3494577026655139\n",
      "logistic losses: [[0.3494577]]\n",
      "logistic losses: [[0.34574162]]\n",
      "train losses\n",
      "logistic losses: [[0.3499134]]\n",
      "0.34991340209336613\n",
      "logistic losses: [[0.3499134]]\n",
      "logistic losses: [[0.34548521]]\n",
      "train losses\n",
      "logistic losses: [[0.35028281]]\n",
      "0.3502828114007154\n",
      "logistic losses: [[0.35028281]]\n",
      "logistic losses: [[0.34521325]]\n",
      "train losses\n",
      "logistic losses: [[0.35057428]]\n",
      "0.3505742778477824\n",
      "logistic losses: [[0.35057428]]\n",
      "logistic losses: [[0.34492731]]\n",
      "train losses\n",
      "logistic losses: [[0.35079537]]\n",
      "0.3507953709093441\n",
      "logistic losses: [[0.35079537]]\n",
      "logistic losses: [[0.34462882]]\n",
      "train losses\n",
      "logistic losses: [[0.35095295]]\n",
      "0.3509529503365544\n",
      "logistic losses: [[0.35095295]]\n",
      "logistic losses: [[0.34431909]]\n",
      "train losses\n",
      "logistic losses: [[0.35105323]]\n",
      "0.35105322915739656\n",
      "logistic losses: [[0.35105323]]\n",
      "logistic losses: [[0.34399925]]\n",
      "train losses\n",
      "logistic losses: [[0.34778021]]\n",
      "0.3477802133503414\n",
      "logistic losses: [[0.34778021]]\n",
      "logistic losses: [[0.3462161]]\n",
      "train losses\n",
      "logistic losses: [[0.34879691]]\n",
      "0.3487969128297462\n",
      "logistic losses: [[0.34879691]]\n",
      "logistic losses: [[0.34585839]]\n",
      "train losses\n",
      "logistic losses: [[0.34964539]]\n",
      "0.3496453911313997\n",
      "logistic losses: [[0.34964539]]\n",
      "logistic losses: [[0.34550054]]\n",
      "train losses\n",
      "logistic losses: [[0.35034499]]\n",
      "0.3503449896000339\n",
      "logistic losses: [[0.35034499]]\n",
      "logistic losses: [[0.34514261]]\n",
      "train losses\n",
      "logistic losses: [[0.35091292]]\n",
      "0.3509129193433467\n",
      "logistic losses: [[0.35091292]]\n",
      "logistic losses: [[0.34478466]]\n",
      "train losses\n",
      "logistic losses: [[0.35136448]]\n",
      "0.3513644760753689\n",
      "logistic losses: [[0.35136448]]\n",
      "logistic losses: [[0.34442674]]\n",
      "train losses\n",
      "logistic losses: [[0.35171324]]\n",
      "0.3517132382128596\n",
      "logistic losses: [[0.35171324]]\n",
      "logistic losses: [[0.34406889]]\n",
      "train losses\n",
      "logistic losses: [[0.35197125]]\n",
      "0.3519712481806081\n",
      "logistic losses: [[0.35197125]]\n",
      "logistic losses: [[0.34371115]]\n",
      "train losses\n",
      "logistic losses: [[0.35214918]]\n",
      "0.3521491773631466\n",
      "logistic losses: [[0.35214918]]\n",
      "logistic losses: [[0.34335356]]\n",
      "train losses\n",
      "logistic losses: [[0.35225648]]\n",
      "0.35225647545193495\n",
      "logistic losses: [[0.35225648]]\n",
      "logistic losses: [[0.34299614]]\n",
      "train losses\n",
      "logistic losses: [[0.3475882]]\n",
      "0.34758819801320096\n",
      "logistic losses: [[0.3475882]]\n",
      "logistic losses: [[0.34611017]]\n",
      "train losses\n",
      "logistic losses: [[0.34845509]]\n",
      "0.3484550867128646\n",
      "logistic losses: [[0.34845509]]\n",
      "logistic losses: [[0.34563544]]\n",
      "train losses\n",
      "logistic losses: [[0.34918909]]\n",
      "0.34918908948015215\n",
      "logistic losses: [[0.34918909]]\n",
      "logistic losses: [[0.34515067]]\n",
      "train losses\n",
      "logistic losses: [[0.34980363]]\n",
      "0.3498036267594512\n",
      "logistic losses: [[0.34980363]]\n",
      "logistic losses: [[0.34465704]]\n",
      "train losses\n",
      "logistic losses: [[0.35031083]]\n",
      "0.350310826206855\n",
      "logistic losses: [[0.35031083]]\n",
      "logistic losses: [[0.34415556]]\n",
      "train losses\n",
      "logistic losses: [[0.35072164]]\n",
      "0.35072163568599146\n",
      "logistic losses: [[0.35072164]]\n",
      "logistic losses: [[0.34364717]]\n",
      "train losses\n",
      "logistic losses: [[0.35104593]]\n",
      "0.3510459289451393\n",
      "logistic losses: [[0.35104593]]\n",
      "logistic losses: [[0.34313269]]\n",
      "train losses\n",
      "logistic losses: [[0.3512926]]\n",
      "0.3512926038186907\n",
      "logistic losses: [[0.3512926]]\n",
      "logistic losses: [[0.34261287]]\n",
      "train losses\n",
      "logistic losses: [[0.35146967]]\n",
      "0.3514696730038919\n",
      "logistic losses: [[0.35146967]]\n",
      "logistic losses: [[0.34208837]]\n",
      "train losses\n",
      "logistic losses: [[0.35158435]]\n",
      "0.3515843476097328\n",
      "logistic losses: [[0.35158435]]\n",
      "logistic losses: [[0.34155978]]\n",
      "train losses\n",
      "logistic losses: [[0.34785399]]\n",
      "0.34785398958949\n",
      "logistic losses: [[0.34785399]]\n",
      "logistic losses: [[0.34621957]]\n",
      "train losses\n",
      "logistic losses: [[0.34889803]]\n",
      "0.34889803252502666\n",
      "logistic losses: [[0.34889803]]\n",
      "logistic losses: [[0.34587151]]\n",
      "train losses\n",
      "logistic losses: [[0.34973791]]\n",
      "0.34973791297928325\n",
      "logistic losses: [[0.34973791]]\n",
      "logistic losses: [[0.3455286]]\n",
      "train losses\n",
      "logistic losses: [[0.35040161]]\n",
      "0.350401608150692\n",
      "logistic losses: [[0.35040161]]\n",
      "logistic losses: [[0.34519018]]\n",
      "train losses\n",
      "logistic losses: [[0.35091339]]\n",
      "0.3509133881402924\n",
      "logistic losses: [[0.35091339]]\n",
      "logistic losses: [[0.34485564]]\n",
      "train losses\n",
      "logistic losses: [[0.35129428]]\n",
      "0.35129427633586985\n",
      "logistic losses: [[0.35129428]]\n",
      "logistic losses: [[0.34452449]]\n",
      "train losses\n",
      "logistic losses: [[0.35156246]]\n",
      "0.35156246141723313\n",
      "logistic losses: [[0.35156246]]\n",
      "logistic losses: [[0.34419629]]\n",
      "train losses\n",
      "logistic losses: [[0.35173366]]\n",
      "0.3517336633646837\n",
      "logistic losses: [[0.35173366]]\n",
      "logistic losses: [[0.34387068]]\n",
      "train losses\n",
      "logistic losses: [[0.35182146]]\n",
      "0.35182145672285137\n",
      "logistic losses: [[0.35182146]]\n",
      "logistic losses: [[0.34354732]]\n",
      "train losses\n",
      "logistic losses: [[0.35183755]]\n",
      "0.3518375547879256\n",
      "logistic losses: [[0.35183755]]\n",
      "logistic losses: [[0.34322596]]\n",
      "train losses\n",
      "logistic losses: [[0.34694887]]\n",
      "0.34694887471845137\n",
      "logistic losses: [[0.34694887]]\n",
      "logistic losses: [[0.34631763]]\n",
      "train losses\n",
      "logistic losses: [[0.34725186]]\n",
      "0.3472518577551043\n",
      "logistic losses: [[0.34725186]]\n",
      "logistic losses: [[0.34605014]]\n",
      "train losses\n",
      "logistic losses: [[0.34748883]]\n",
      "0.34748883051781887\n",
      "logistic losses: [[0.34748883]]\n",
      "logistic losses: [[0.3457722]]\n",
      "train losses\n",
      "logistic losses: [[0.34766556]]\n",
      "0.34766555981584335\n",
      "logistic losses: [[0.34766556]]\n",
      "logistic losses: [[0.34548477]]\n",
      "train losses\n",
      "logistic losses: [[0.34778733]]\n",
      "0.3477873288293085\n",
      "logistic losses: [[0.34778733]]\n",
      "logistic losses: [[0.34518874]]\n",
      "train losses\n",
      "logistic losses: [[0.34785898]]\n",
      "0.3478589751937609\n",
      "logistic losses: [[0.34785898]]\n",
      "logistic losses: [[0.34488492]]\n",
      "train losses\n",
      "logistic losses: [[0.34788493]]\n",
      "0.3478849265361186\n",
      "logistic losses: [[0.34788493]]\n",
      "logistic losses: [[0.34457404]]\n",
      "train losses\n",
      "logistic losses: [[0.34786923]]\n",
      "0.3478692335441388\n",
      "logistic losses: [[0.34786923]]\n",
      "logistic losses: [[0.34425678]]\n",
      "train losses\n",
      "logistic losses: [[0.3478156]]\n",
      "0.34781560066944295\n",
      "logistic losses: [[0.3478156]]\n",
      "logistic losses: [[0.34393375]]\n",
      "train losses\n",
      "logistic losses: [[0.34772741]]\n",
      "0.34772741457602696\n",
      "logistic losses: [[0.34772741]]\n",
      "logistic losses: [[0.34360551]]\n",
      "train losses\n",
      "logistic losses: [[0.34720495]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3472049510064285\n",
      "logistic losses: [[0.34720495]]\n",
      "logistic losses: [[0.34612931]]\n",
      "train losses\n",
      "logistic losses: [[0.34775385]]\n",
      "0.3477538492809593\n",
      "logistic losses: [[0.34775385]]\n",
      "logistic losses: [[0.34567528]]\n",
      "train losses\n",
      "logistic losses: [[0.34822654]]\n",
      "0.3482265397599832\n",
      "logistic losses: [[0.34822654]]\n",
      "logistic losses: [[0.34521234]]\n",
      "train losses\n",
      "logistic losses: [[0.34862883]]\n",
      "0.3486288306216481\n",
      "logistic losses: [[0.34862883]]\n",
      "logistic losses: [[0.34474126]]\n",
      "train losses\n",
      "logistic losses: [[0.34896611]]\n",
      "0.34896611163310143\n",
      "logistic losses: [[0.34896611]]\n",
      "logistic losses: [[0.34426278]]\n",
      "train losses\n",
      "logistic losses: [[0.34924338]]\n",
      "0.34924338110802466\n",
      "logistic losses: [[0.34924338]]\n",
      "logistic losses: [[0.34377755]]\n",
      "train losses\n",
      "logistic losses: [[0.34946527]]\n",
      "0.3494652716690302\n",
      "logistic losses: [[0.34946527]]\n",
      "logistic losses: [[0.34328618]]\n",
      "train losses\n",
      "logistic losses: [[0.34963607]]\n",
      "0.34963607476369873\n",
      "logistic losses: [[0.34963607]]\n",
      "logistic losses: [[0.34278924]]\n",
      "train losses\n",
      "logistic losses: [[0.34975976]]\n",
      "0.3497597639101191\n",
      "logistic losses: [[0.34975976]]\n",
      "logistic losses: [[0.34228725]]\n",
      "train losses\n",
      "logistic losses: [[0.34984002]]\n",
      "0.3498400166690493\n",
      "logistic losses: [[0.34984002]]\n",
      "logistic losses: [[0.34178068]]\n",
      "train losses\n",
      "logistic losses: [[0.3477632]]\n",
      "0.3477631986781117\n",
      "logistic losses: [[0.3477632]]\n",
      "logistic losses: [[0.34624418]]\n",
      "train losses\n",
      "logistic losses: [[0.34876916]]\n",
      "0.3487691575421733\n",
      "logistic losses: [[0.34876916]]\n",
      "logistic losses: [[0.34591611]]\n",
      "train losses\n",
      "logistic losses: [[0.34961197]]\n",
      "0.34961196748733736\n",
      "logistic losses: [[0.34961197]]\n",
      "logistic losses: [[0.34558924]]\n",
      "train losses\n",
      "logistic losses: [[0.35030995]]\n",
      "0.3503099531102444\n",
      "logistic losses: [[0.35030995]]\n",
      "logistic losses: [[0.34526348]]\n",
      "train losses\n",
      "logistic losses: [[0.35087947]]\n",
      "0.35087946983972773\n",
      "logistic losses: [[0.35087947]]\n",
      "logistic losses: [[0.34493873]]\n",
      "train losses\n",
      "logistic losses: [[0.3513351]]\n",
      "0.3513350971055667\n",
      "logistic losses: [[0.3513351]]\n",
      "logistic losses: [[0.34461491]]\n",
      "train losses\n",
      "logistic losses: [[0.35168982]]\n",
      "0.35168981705967095\n",
      "logistic losses: [[0.35168982]]\n",
      "logistic losses: [[0.34429195]]\n",
      "train losses\n",
      "logistic losses: [[0.35195518]]\n",
      "0.3519551787149444\n",
      "logistic losses: [[0.35195518]]\n",
      "logistic losses: [[0.34396977]]\n",
      "train losses\n",
      "logistic losses: [[0.35214145]]\n",
      "0.3521414477986815\n",
      "logistic losses: [[0.35214145]]\n",
      "logistic losses: [[0.34364833]]\n",
      "train losses\n",
      "logistic losses: [[0.35225774]]\n",
      "0.35225774290231343\n",
      "logistic losses: [[0.35225774]]\n",
      "logistic losses: [[0.34332757]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "# n_components = [20, 30, 40]\n",
    "# learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05] # 10] # 0.01 - 1\n",
    "\n",
    "n_components = [20]\n",
    "learning_rates = [0.06]\n",
    "n_epoches = 10\n",
    "for n_comp in n_components:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "#         pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"softmax\")\n",
    "        pipeline = Pipeline(facial_expressions=['ht','m'], classifier_type=\"logistic\")\n",
    "        pipeline.build(n_components=n_comp, learning_rate=lr, n_epoches=n_epoches, batch_size=None, n_repeats=1)\n",
    "        pipeline.run()\n",
    "        \n",
    "        # plot errors\n",
    "        pipeline.records.plt_losses(n_components=n_comp, lr=lr, n_epoches=n_epoches)\n",
    "        pipeline.records.show_accuracies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
